---
title: "Artificial Intelligence (H) Final Review Note"
tags:
- Artificial Intelligence 
- Review Note
excerpt: "Final review note for CS311 Artificial Intelligence (H)"
---

<!DOCTYPE html><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>1 Intelligent Agents</title>
<!--Generated on Sat Jul 27 23:19:37 2024 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="{{ site.baseurl }}/assets/css/LaTeXML.css" type="text/css">
<link rel="stylesheet" href="{{ site.baseurl }}/assets/css/ltx-article.css" type="text/css">
<link rel="stylesheet" href="{{ site.baseurl }}/assets/css/ltx-ulem.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Intelligent Agents</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Definition</h3>

<section id="S1.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Agent</h5>

<div id="S1.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">An <span class="ltx_text ltx_font_bold">agent</span> is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. An agent program runs in cycles of perceive, think, and act. An <span class="ltx_text ltx_font_bold">agent function</span> is a mapping from percepts to actions. An agent is a combination of an architecture and a program, which should be complementary and compatible.</p>
</div>
</section>
<section id="S1.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Rational Agent</h5>

<div id="S1.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For each possible percept sequence, a <span class="ltx_text ltx_font_bold">rational agent</span> should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.</p>
</div>
</section>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>PEAS</h3>

<div id="S1.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">When we define a rational agent, we group the agent’s performance elements into four categories: <span class="ltx_text ltx_font_bold">Performance measure</span>, <span class="ltx_text ltx_font_bold">Environment</span>, <span class="ltx_text ltx_font_bold">Actuators</span>, and <span class="ltx_text ltx_font_bold">Sensors</span>. This is called the PEAS description.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">Example:</span>
</p>
</div>
<div id="S1.SS2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">For a self-driving car, the PEAS description is as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Performance measure</span>: Safety, legality, passenger comfort, and arrival time.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Environment</span>: Roads, traffic, pedestrians, other cars, weather.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Actuators</span>: Steering wheel, accelerator, brake, signal lights, horn.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Sensors</span>: Cameras, LIDAR, GPS, speedometer, odometer, engine sensors.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Properties of Environments</h3>

<section id="S1.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fully Observable vs. Partially Observable</h5>

<div id="S1.SS3.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">An environment is <span class="ltx_text ltx_font_bold">fully observable</span> if the agent’s sensors give it access to the complete state of the environment at each point in time. Otherwise, the environment is <span class="ltx_text ltx_font_bold">partially observable</span>.</p>
</div>
</section>
<section id="S1.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deterministic vs. Stochastic</h5>

<div id="S1.SS3.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the next state of the environment is completely determined by the current state and the action executed by the agent, the environment is <span class="ltx_text ltx_font_bold">deterministic</span>. Otherwise, the environment is <span class="ltx_text ltx_font_bold">stochastic</span>.</p>
</div>
</section>
<section id="S1.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Episodic vs. Sequential</h5>

<div id="S1.SS3.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The agent’s experience is divided into episodes. Each episode consists of the agent perceiving and then acting. The choice of action in each episode depends only on the episode itself. In a <span class="ltx_text ltx_font_bold">sequential</span> environment, the current decision could affect all future decisions.</p>
</div>
</section>
<section id="S1.SS3.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Static vs. Dynamic</h5>

<div id="S1.SS3.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the environment can change while an agent is deliberating (i.e., the environment does not wait for the agent to make a decision), the environment is <span class="ltx_text ltx_font_bold">dynamic</span>. Otherwise, it is <span class="ltx_text ltx_font_bold">static</span>.</p>
</div>
</section>
<section id="S1.SS3.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Discrete vs. Continuous</h5>

<div id="S1.SS3.SSS0.Px5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A discrete environment has a finite number of percepts and actions. A continuous environment has a continuous range of percepts and actions.</p>
</div>
</section>
<section id="S1.SS3.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Single-agent vs. Multi-agent</h5>

<div id="S1.SS3.SSS0.Px6.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A single-agent environment contains exactly one agent. A multiagent environment contains multiple agents.</p>
</div>
</section>
<section id="S1.SS3.SSS0.Px7" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Known vs. Unknown</h5>

<div id="S1.SS3.SSS0.Px7.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the designer of the agent have knowledge of the environment, the environment is <span class="ltx_text ltx_font_bold">known</span>. Otherwise, the environment is <span class="ltx_text ltx_font_bold">unknown</span>.</p>
</div>
<div id="S1.SS3.SSS0.Px7.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">Example:</span>
</p>
</div>
<div id="S1.SS3.SSS0.Px7.p3" class="ltx_para">
<p class="ltx_p">Consider the following environments:</p>
</div>
<figure id="S1.SS3.SSS0.Px7.tab1" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Environment</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Observable</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Deterministic</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Agent</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Static</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Discrete</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">8-puzzle</th>
<td class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_t">Single-agent</td>
<td class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_t">Yes</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Chess</th>
<td class="ltx_td ltx_align_center">Yes</td>
<td class="ltx_td ltx_align_center">Yes</td>
<td class="ltx_td ltx_align_center">Multi-agent</td>
<td class="ltx_td ltx_align_center">Yes</td>
<td class="ltx_td ltx_align_center">Yes</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Poker</th>
<td class="ltx_td ltx_align_center">No</td>
<td class="ltx_td ltx_align_center">No</td>
<td class="ltx_td ltx_align_center">Multi-agent</td>
<td class="ltx_td ltx_align_center">Yes</td>
<td class="ltx_td ltx_align_center">Yes</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Car</th>
<td class="ltx_td ltx_align_center ltx_border_bb">No</td>
<td class="ltx_td ltx_align_center ltx_border_bb">No</td>
<td class="ltx_td ltx_align_center ltx_border_bb">Multi-agent</td>
<td class="ltx_td ltx_align_center ltx_border_bb">No</td>
<td class="ltx_td ltx_align_center ltx_border_bb">No</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Types of Agents</h3>

<section id="S1.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.4.1 </span>Simple Reflex Agents</h4>

<div id="S1.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A <span class="ltx_text ltx_font_bold">simple reflex agent</span> selects actions on the basis of the current percept, ignoring the rest of the percept history. It works well in fully observable, deterministic, and static environments with a few states.</p>
</div>
<div id="S1.SS4.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Based on its definition, a simple reflex agent only works well in environments where the current percept is sufficient to determine the correct action. In other words, the environment must be fully observable.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering" width="325" height="137" alt="A simple reflex agent.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A simple reflex agent.</figcaption>
</figure>
<div id="S1.SS4.SSS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">Example:</span>
</p>
</div>
<div id="S1.SS4.SSS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">Consider a simple reflex agent for a vacuum cleaner. The agent has two rules:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the current location is dirty, then clean.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the current location is clean, then move to the other location.</p>
</div>
</li>
</ul>
<p class="ltx_p">This agent works well in a fully observable, deterministic, and static environment with a few states.</p>
</div>
</section>
<section id="S1.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.4.2 </span>Model-based Reflex Agents</h4>

<div id="S1.SS4.SSS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A <span class="ltx_text ltx_font_bold">model-based reflex agent</span> maintains an internal state that depends on the percept history and thereby can handle partially observable environments. In other words, a model-based reflex agent learns how the world evolves over time, and uses this knowledge to predict the part of world that it cannot observe, thus making better decisions.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering" width="325" height="137" alt="A model-based reflex agent.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A model-based reflex agent.</figcaption>
</figure>
</section>
<section id="S1.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.4.3 </span>Goal-based Agents</h4>

<div id="S1.SS4.SSS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A <span class="ltx_text ltx_font_bold">goal-based agent</span> is an agent that plans ahead to achieve its goals. It uses a <span class="ltx_text ltx_font_bold">goal</span> to decide which action to take. And to do so, it must have the ability to predict the consequences of its actions.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig3.png" id="S1.F3.g1" class="ltx_graphics ltx_centering" width="325" height="137" alt="A goal-based agent.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A goal-based agent.</figcaption>
</figure>
</section>
<section id="S1.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.4.4 </span>Utility-based Agents</h4>

<div id="S1.SS4.SSS4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A <span class="ltx_text ltx_font_bold">utility-based agent</span> is an agent that chooses the action that maximizes the expected utility of the agent’s performance measure. It is similar to a goal-based agent, but it uses a <span class="ltx_text ltx_font_bold">utility function</span> to decide which action to take.</p>
</div>
<figure id="S1.F4" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig4.png" id="S1.F4.g1" class="ltx_graphics ltx_centering" width="325" height="200" alt="A utility-based agent.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A utility-based agent.</figcaption>
</figure>
</section>
<section id="S1.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.4.5 </span>Learning Agents</h4>

<div id="S1.SS4.SSS5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A <span class="ltx_text ltx_font_bold">learning agent</span> is a generalized agent that can learn from its experience. It has 4 components: a <span class="ltx_text ltx_font_bold">learning element</span> that is responsible for making improvements, a <span class="ltx_text ltx_font_bold">performance element</span> that selects external actions, a <span class="ltx_text ltx_font_bold">critic</span> that provides feedback on how well the agent is doing, and a <span class="ltx_text ltx_font_bold">problem generator</span> that suggests actions that lead to new and informative experiences.</p>
</div>
</section>
</section>
<section id="S1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.5 </span>Agent States</h3>

<section id="S1.SS5.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Atomic Representation</h5>

<div id="S1.SS5.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">An <span class="ltx_text ltx_font_bold">atomic representation</span> assumes each state of the world is a black box without internal structure. For example, in a board game, each state is a unique configuration of the board.</p>
</div>
</section>
<section id="S1.SS5.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Factored Representation</h5>

<div id="S1.SS5.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A <span class="ltx_text ltx_font_bold">factored representation</span> decomposes the state into independent components. These variables can describe the complete state of the world. For example, in a gird-based world, the state can be decomposed into the position of the agent, positions of obstacles and goals, etc.</p>
</div>
</section>
<section id="S1.SS5.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Structured Representation</h5>

<div id="S1.SS5.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A <span class="ltx_text ltx_font_bold">structured representation</span> captures the relationships between components of the state. For example, a map that shows the distances between cities.</p>
</div>
</section>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Search</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Problem Formulation</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Initial state</span>: The state in which the agent starts.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">States</span>: The set of all reachable states by any sequence of actions.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Actions</span>: The set of all possible actions that the agent can take.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Transition model</span>: A description of what each action does.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Goal test</span>: A function that determines whether a state is a goal state.</p>
</div>
</li>
<li id="S2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i6.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Path cost</span>: A function that assigns a numeric cost to each path.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Search Space</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">State Space vs. Search Space</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">state space</span> is the set of all states reachable from the initial state by any sequence of actions. The <span class="ltx_text ltx_font_bold">search space</span> is an abstract configuration that models the sequence of actions taken by the agent. For example, a search tree is a search space, where its root is the initial state, branches are actions, and nodes result from applying actions to states.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">A search space is divided into three parts: <span class="ltx_text ltx_font_bold">frontier</span>, <span class="ltx_text ltx_font_bold">explored set</span>, and <span class="ltx_text ltx_font_bold">unexplored set</span>. The frontier is the set of nodes that have been generated but not yet expanded. The explored set is the set of nodes that have been expanded. The unexplored set is the set of nodes that have not been generated.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Search Strategies</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Strategies are evaluated based on the following criteria:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Completeness</span>: Does the strategy guarantee to find a solution if one exists?</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Optimality</span>: Does the strategy guarantee to find the optimal solution?</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Time complexity</span>: How long does it take to find a solution?
</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Space complexity</span>: How much memory does it take to find a solution?</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">In particular, time complexity and space complexity are measured in terms of following factors:</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">b</span>: The branching factor, i.e., the maximum number of children of any node.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">d</span>: The depth of the solution.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">m</span>: The maximum depth of the search tree.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">We can classify search strategies into two categories: <span class="ltx_text ltx_font_bold">uninformed search</span> and <span class="ltx_text ltx_font_bold">informed search</span>.</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Uninformed Search</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Breadth-first search (BFS)</span>: Expands the shallowest unexpanded node first.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Depth-first search (DFS)</span>: Expands the deepest unexpanded node first.</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Depth-limited search (DLS)</span>: A variant of DFS that limits the depth of the search.</p>
</div>
</li>
<li id="S2.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Iterative deepening search (IDS)</span>: A variant of DLS that gradually increases the depth limit.</p>
</div>
</li>
<li id="S2.I4.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Uniform-cost search (UCS)</span>: Expands the node with the lowest path cost (the same idea used in Dijkstra’s algorithm).</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para">
<p class="ltx_p">Here are the properties of these strategies:</p>
</div>
<figure id="S2.SS3.SSS1.tab1" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Strategy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Completeness</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Optimality</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Time complexity</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Space complexity</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">BFS</td>
<td class="ltx_td ltx_align_center ltx_border_t">Yes if <math id="S2.SS3.SSS1.m1" class="ltx_Math" alttext="b" display="inline"><mi>b</mi></math> is finite</td>
<td class="ltx_td ltx_align_center ltx_border_t">Yes if cost is the same for every path</td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S2.SS3.SSS1.m2" class="ltx_Math" alttext="O(b^{d})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>b</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S2.SS3.SSS1.m3" class="ltx_Math" alttext="O(b^{d})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>b</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">DFS</td>
<td class="ltx_td ltx_align_center">Yes if the state space is finite</td>
<td class="ltx_td ltx_align_center">No</td>
<td class="ltx_td ltx_align_center"><math id="S2.SS3.SSS1.m4" class="ltx_Math" alttext="O(b^{m})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>b</mi><mi>m</mi></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center"><math id="S2.SS3.SSS1.m5" class="ltx_Math" alttext="O(bm)" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>b</mi><mo>⁢</mo><mi>m</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">UCS</td>
<td class="ltx_td ltx_align_center ltx_border_bb">Yes if the solution has a finite cost</td>
<td class="ltx_td ltx_align_center ltx_border_bb">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math id="S2.SS3.SSS1.m6" class="ltx_Math" alttext="O(b^{1+\lfloor C^{*}/\epsilon\rfloor})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>b</mi><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy="false">⌊</mo><mrow><msup><mi>C</mi><mo>*</mo></msup><mo>/</mo><mi>ϵ</mi></mrow><mo stretchy="false">⌋</mo></mrow></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math id="S2.SS3.SSS1.m7" class="ltx_Math" alttext="O(b^{1+\lfloor C^{*}/\epsilon\rfloor})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>b</mi><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy="false">⌊</mo><mrow><msup><mi>C</mi><mo>*</mo></msup><mo>/</mo><mi>ϵ</mi></mrow><mo stretchy="false">⌋</mo></mrow></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS3.SSS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Note:</span> <math id="S2.SS3.SSS1.p3.m1" class="ltx_Math" alttext="C^{*}" display="inline"><msup><mi>C</mi><mo>*</mo></msup></math> is the cost of the optimal solution, and <math id="S2.SS3.SSS1.p3.m2" class="ltx_Math" alttext="\epsilon" display="inline"><mi>ϵ</mi></math> is the minimum cost of any action.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Informed Search</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<ul id="S2.I5" class="ltx_itemize">
<li id="S2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Greedy best-first search</span>: Expands the node that is closest to the goal.</p>
</div>
</li>
<li id="S2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">A* search</span>: Expands the node that minimizes <math id="S2.I5.i2.p1.m1" class="ltx_Math" alttext="f(n)=g(n)+h(n)" display="inline"><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>g</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>, where <math id="S2.I5.i2.p1.m2" class="ltx_Math" alttext="g(n)" display="inline"><mrow><mi>g</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></math> is the cost to reach <math id="S2.I5.i2.p1.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> from the initial state, and <math id="S2.I5.i2.p1.m4" class="ltx_Math" alttext="h(n)" display="inline"><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></math> is the estimated cost to reach the goal from <math id="S2.I5.i2.p1.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>.</p>
</div>
</li>
<li id="S2.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">IDA* search</span>: A variant of A* that uses iterative deepening.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The quality of informed search strategies depends on the quality of the heuristic function.</p>
<ul id="S2.I6" class="ltx_itemize">
<li id="S2.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I6.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Admissible heuristic</span>: A heuristic function is admissible if it never overestimates the cost to reach the goal. In other words, <math id="S2.I6.i1.p1.m1" class="ltx_Math" alttext="h(n)\leq h^{*}(n)" display="inline"><mrow><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, where <math id="S2.I6.i1.p1.m2" class="ltx_Math" alttext="h^{*}(n)" display="inline"><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></math> is the true cost to reach the goal from <math id="S2.I6.i1.p1.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>. An admissible heuristic guarantees to find the optimal solution.</p>
</div>
</li>
<li id="S2.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I6.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Consistent heuristic</span>: A heuristic function is consistent if for every node <math id="S2.I6.i2.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> and every successor <math id="S2.I6.i2.p1.m2" class="ltx_Math" alttext="n^{\prime}" display="inline"><msup><mi>n</mi><mo>′</mo></msup></math> of <math id="S2.I6.i2.p1.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> generated by any action <math id="S2.I6.i2.p1.m4" class="ltx_Math" alttext="a" display="inline"><mi>a</mi></math>, <math id="S2.I6.i2.p1.m5" class="ltx_Math" alttext="h(n)\leq c(n,a,n^{\prime})+h(n^{\prime})" display="inline"><mrow><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>n</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">More Info:</span>
</p>
</div>
<div id="S2.SS3.SSS2.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">The closer the heuristic function is to the true cost, the more efficient the search strategy will be.</p>
<ul id="S2.I7" class="ltx_itemize">
<li id="S2.I7.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I7.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If <math id="S2.I7.i1.p1.m1" class="ltx_Math" alttext="h(n)=0" display="inline"><mrow><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>, A* search degenerates into UCS.</p>
</div>
</li>
<li id="S2.I7.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I7.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If <math id="S2.I7.i2.p1.m1" class="ltx_Math" alttext="h(n)=h^{*}(n)" display="inline"><mrow><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, we no longer need to find the distance by searching, since <math id="S2.I7.i2.p1.m2" class="ltx_Math" alttext="h(n_{\text{start}})" display="inline"><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>n</mi><mtext>start</mtext></msub><mo stretchy="false">)</mo></mrow></mrow></math> already gives the distance to the goal.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Local Search</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Definition</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The search strategies we have discussed so far are called <span class="ltx_text ltx_font_bold">systematic search</span>. For these strategies, when the goal is reached, a solution that consists of a sequence of actions is found.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">However, sometimes we do not need to know how to reach the goal (i.e., the sequence of actions). Instead, we only need to know the state that satisfies the goal. In this case, we can use <span class="ltx_text ltx_font_bold">local search</span>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">Example:</span>
</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">AI training is a good example of local search. All we care about is the final model that has the best performance. We do not need to know how the model is trained.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">The basic idea of local search is to keep a few states in memory and iteratively improve them. This means we only longer need to store the entire search tree, which saves memory.
</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p class="ltx_p">Although local search is not guaranteed to find the optimal solution, it often can find a good solution in a reasonable amount of time.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Local Search Strategies</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Hill Climbing</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hill climbing</span> is a simple local search strategy that iteratively improves the current state by moving to the neighboring state with the highest value. However, this might cause the agent to get stuck in a local maximum or a plateau.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Some variants of hill climbing include:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Sideways move</span>: Allow the agent to move to a neighboring state with the same value. This allows the agent to escape plateaus, but not local maxima.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Random-restart hill climbing</span>: Restart the search from a random state.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Stochastic hill climbing</span>: Randomly select the next state among the neighboring states.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Local beam search</span>: Keep <math id="S3.I1.i4.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> states in memory and expand them simultaneously.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Stochastic beam search</span>: Randomly select <math id="S3.I1.i5.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> states among the neighboring states.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Simulated Annealing</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Simulated annealing</span> is a variant of hill climbing that allows the agent to move to a neighboring state with a lower value with a certain probability. This probability decreases over time. This allows the agent to escape local maxima.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The probability of moving to a neighboring state with a lower value is given by:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1" class="ltx_Math" alttext="P(\text{move to }n^{\prime})=\begin{cases}1&amp;\text{if }\Delta E&lt;0\\
e^{-\frac{\Delta E}{T}}&amp;\text{otherwise}\end{cases}" display="block"><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mtext>move to </mtext><mo>⁢</mo><msup><mi>n</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd columnalign="left"><mn>1</mn></mtd><mtd columnalign="left"><mrow><mrow><mtext>if </mtext><mo>⁢</mo><mi mathvariant="normal">Δ</mi><mo>⁢</mo><mi>E</mi></mrow><mo>&lt;</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><msup><mi>e</mi><mrow><mo>-</mo><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><mi>E</mi></mrow><mi>T</mi></mfrac></mrow></msup></mtd><mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S3.SS2.SSS2.p2.m1" class="ltx_Math" alttext="\Delta E=V(n)-V(n^{\prime})" display="inline"><mrow><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><mi>E</mi></mrow><mo>=</mo><mrow><mrow><mi>V</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>-</mo><mrow><mi>V</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math> is the difference in value between the current state and the neighboring state, and <math id="S3.SS2.SSS2.p2.m2" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> is the temperature.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Genetic Algorithms</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Genetic algorithms</span> is a variant of stochastic beam search. It is inspired by the process of natural selection. The algorithm maintains a population of states. Successor states are generated by combining two parent states. The algorithm then selects a new population by evaluating the fitness of each state.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The major improvement of genetic algorithms is that produce new states by combining two parent states, not by modifying a single state. This allows the algorithm to explore a larger search space.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Better Heuristics*</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Dominance</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The quality of local search strategies depends on the quality of the heuristic function. In the previous section, we discussed the admissible and consistent heuristic functions. However, only looking at these two properties is not enough, because we can easily find many admissible and consistent heuristic functions. To compare different admissible heuristic functions, we consider <span class="ltx_text ltx_font_bold">dominance</span>.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">We say that a heuristic function <math id="S3.SS3.SSS1.p2.m1" class="ltx_Math" alttext="h_{1}" display="inline"><msub><mi>h</mi><mn>1</mn></msub></math> <span class="ltx_text ltx_font_bold">dominates</span> another heuristic function <math id="S3.SS3.SSS1.p2.m2" class="ltx_Math" alttext="h_{2}" display="inline"><msub><mi>h</mi><mn>2</mn></msub></math> if <math id="S3.SS3.SSS1.p2.m3" class="ltx_Math" alttext="h_{1}(n)\geq h_{2}(n)" display="inline"><mrow><mrow><msub><mi>h</mi><mn>1</mn></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><msub><mi>h</mi><mn>2</mn></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math> for all nodes <math id="S3.SS3.SSS1.p2.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>. In other words, <math id="S3.SS3.SSS1.p2.m5" class="ltx_Math" alttext="h_{1}" display="inline"><msub><mi>h</mi><mn>1</mn></msub></math> is better than <math id="S3.SS3.SSS1.p2.m6" class="ltx_Math" alttext="h_{2}" display="inline"><msub><mi>h</mi><mn>2</mn></msub></math> if <math id="S3.SS3.SSS1.p2.m7" class="ltx_Math" alttext="h_{1}" display="inline"><msub><mi>h</mi><mn>1</mn></msub></math> is always greater than or equal to <math id="S3.SS3.SSS1.p2.m8" class="ltx_Math" alttext="h_{2}" display="inline"><msub><mi>h</mi><mn>2</mn></msub></math>.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">More Info:</span>
</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">From a collection of admissible heuristic functions <math id="S3.SS3.SSS1.p4.m1" class="ltx_Math" alttext="h_{1},h_{2},\ldots,h_{n}" display="inline"><mrow><msub><mi>h</mi><mn>1</mn></msub><mo>,</mo><msub><mi>h</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>h</mi><mi>n</mi></msub></mrow></math>, we can construct a new heuristic function <math id="S3.SS3.SSS1.p4.m2" class="ltx_Math" alttext="h(n)=\max\{h_{1}(n),h_{2}(n),\ldots,h_{n}(n)\}" display="inline"><mrow><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mrow><msub><mi>h</mi><mn>1</mn></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>h</mi><mn>2</mn></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msub><mi>h</mi><mi>n</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow></math>. It is easy to see that <math id="S3.SS3.SSS1.p4.m3" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math> dominates all <math id="S3.SS3.SSS1.p4.m4" class="ltx_Math" alttext="h_{i}" display="inline"><msub><mi>h</mi><mi>i</mi></msub></math>.
</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Methods to Find Good Heuristics</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Relaxation</span>: Relax the problem by removing constraints. For example, in the 8-puzzle, we can relax the constraint that a tile can only move to adjacent empty cells, and allow it to move to any empty cell. Or we can relax the constraint that a tile can move to an occupied cell.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Generated from sub-problems</span>: Only solve a part of the problem, and use the cost of the solution as the heuristic function. For example, in the 8-puzzle, we can solve the problem for half of the tiles, and use the cost of the solution as the heuristic function.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Generated from experience</span>: Use the cost of the solution of similar problems as the heuristic function. For example, in the 8-puzzle, we can use the cost of the solution of similar 8-puzzles as the heuristic function.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Adversarial Search</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In Adversarial Search, we have an additional utility function that assigns a numeric value to each terminal state for each player. The goal of each player is to maximize its utility. If the sum of the utilities of all players is 0, the game is called a <span class="ltx_text ltx_font_bold">zero-sum game</span>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Minimax Algorithm</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">minimax algorithm</span> is a search algorithm that is used to find the optimal strategy for a player in a two-player zero-sum game. The algorithm is based on the following idea: if both players play optimally, each player will choose the action that maximizes his utility, and assume that the opponent will choose the action that minimizes his utility (to be noticed that here two “his” represent the same player).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">For a state <math id="S4.SS1.p2.m1" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>, <math id="S4.SS1.p2.m2" class="ltx_Math" alttext="minimax(s)" display="inline"><mrow><mi>m</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></math> is:</p>
<table id="S4.Ex2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex2.m1" class="ltx_Math" alttext="\text{minimax}(s)=\begin{cases}\text{utility}(s)&amp;\text{if }\text{terminal}(s)%
\\
\max_{a\in\text{actions}(s)}\text{minimax}(\text{result}(s,a))&amp;\text{if }\text%
{player}(s)=\text{MAX}\\
\min_{a\in\text{actions}(s)}\text{minimax}(\text{result}(s,a))&amp;\text{if }\text%
{player}(s)=\text{MIN}\end{cases}" display="block"><mrow><mrow><mtext>minimax</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd columnalign="left"><mrow><mtext>utility</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mtext>if terminal</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mrow><msub><mi>max</mi><mrow><mi>a</mi><mo>∈</mo><mrow><mtext>actions</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo>⁡</mo><mtext>minimax</mtext></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mtext>result</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mrow><mtext>if player</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mtext>MAX</mtext></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mrow><msub><mi>min</mi><mrow><mi>a</mi><mo>∈</mo><mrow><mtext>actions</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo>⁡</mo><mtext>minimax</mtext></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mtext>result</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mrow><mtext>if player</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mtext>MIN</mtext></mrow></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">which represents the maximum utility that can be obtained from state <math id="S4.SS1.p2.m3" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering" width="325" height="140" alt="An example of the minimax algorithm, where the red nodes represent the MAX player, and the blue nodes represent the MIN player.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>An example of the minimax algorithm, where the red nodes represent the MAX player, and the blue nodes represent the MIN player.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">We can observe that the tree is build bottom-up. The algorithm starts from the terminal states and propagates the utility values up to the root.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">However, it is not practical to search the entire game tree. And some techniques can be used to improve the performance of the minimax algorithm.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Alpha-Beta Pruning</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Alpha-beta pruning</span> is a technique that reduces the number of nodes that need to be evaluated in the minimax algorithm. The idea is that, since the MAX player will choose the maximum value among the children, if we know the value of certain subtree will never be the maximum value, then we do not need to explore it, and same for the MIN player.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p class="ltx_p">For example, below displays an incomplete DFS search tree, where the black nodes are pruned.</p>
</div>
<figure id="S4.SS1.SSS1.fig1" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig6.png" id="S4.SS1.SSS1.g1" class="ltx_graphics ltx_centering" width="325" height="105" alt="">
</figure>
<div id="S4.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">In this example, we discovered that <math id="S4.SS1.SSS1.p3.m1" class="ltx_Math" alttext="v_{21}=2" display="inline"><mrow><msub><mi>v</mi><mn>21</mn></msub><mo>=</mo><mn>2</mn></mrow></math>. Since MIN player decide on the value of <math id="S4.SS1.SSS1.p3.m2" class="ltx_Math" alttext="v_{2}" display="inline"><msub><mi>v</mi><mn>2</mn></msub></math>, he will choose the smallest value among <math id="S4.SS1.SSS1.p3.m3" class="ltx_Math" alttext="v_{21}" display="inline"><msub><mi>v</mi><mn>21</mn></msub></math>, <math id="S4.SS1.SSS1.p3.m4" class="ltx_Math" alttext="v_{22}" display="inline"><msub><mi>v</mi><mn>22</mn></msub></math>, and <math id="S4.SS1.SSS1.p3.m5" class="ltx_Math" alttext="v_{23}" display="inline"><msub><mi>v</mi><mn>23</mn></msub></math>. Since <math id="S4.SS1.SSS1.p3.m6" class="ltx_Math" alttext="v_{21}=2" display="inline"><mrow><msub><mi>v</mi><mn>21</mn></msub><mo>=</mo><mn>2</mn></mrow></math>, the value of <math id="S4.SS1.SSS1.p3.m7" class="ltx_Math" alttext="v_{2}" display="inline"><msub><mi>v</mi><mn>2</mn></msub></math> will never be greater than 2. Then, when MAX player decide on the value of <math id="S4.SS1.SSS1.p3.m8" class="ltx_Math" alttext="r" display="inline"><mi>r</mi></math>, he will choose the largest value among <math id="S4.SS1.SSS1.p3.m9" class="ltx_Math" alttext="v_{1}" display="inline"><msub><mi>v</mi><mn>1</mn></msub></math>, <math id="S4.SS1.SSS1.p3.m10" class="ltx_Math" alttext="v_{2}" display="inline"><msub><mi>v</mi><mn>2</mn></msub></math> and <math id="S4.SS1.SSS1.p3.m11" class="ltx_Math" alttext="v_{3}" display="inline"><msub><mi>v</mi><mn>3</mn></msub></math>. Since <math id="S4.SS1.SSS1.p3.m12" class="ltx_Math" alttext="v_{2}\leq 2" display="inline"><mrow><msub><mi>v</mi><mn>2</mn></msub><mo>≤</mo><mn>2</mn></mrow></math> and <math id="S4.SS1.SSS1.p3.m13" class="ltx_Math" alttext="v_{1}=3" display="inline"><mrow><msub><mi>v</mi><mn>1</mn></msub><mo>=</mo><mn>3</mn></mrow></math>, he will never choose <math id="S4.SS1.SSS1.p3.m14" class="ltx_Math" alttext="v_{2}" display="inline"><msub><mi>v</mi><mn>2</mn></msub></math>. Therefore, we can prune the subtree rooted at <math id="S4.SS1.SSS1.p3.m15" class="ltx_Math" alttext="v_{2}" display="inline"><msub><mi>v</mi><mn>2</mn></msub></math>.</p>
</div>
<div id="S4.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_framed_rectangle" style="border-color: black;">
<span class="ltx_text ltx_font_bold">More Info:</span>
</span></p>
</div>
<div id="S4.SS1.SSS1.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">Formally, <math id="S4.SS1.SSS1.p5.m1" class="ltx_Math" alttext="minimax(r)" display="inline"><mrow><mi>m</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo></mrow></mrow></math> is:</p>
<table id="S14.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex3.m1" class="ltx_Math" alttext="\displaystyle\text{minimax}(r)" display="inline"><mrow><mtext>minimax</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.Ex3.m2" class="ltx_Math" alttext="\displaystyle=\max\{\min\{3,9,8\},\min\{2,x,y\},\ldots\}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mn>3</mn><mo>,</mo><mn>9</mn><mo>,</mo><mn>8</mn><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mn>2</mn><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo stretchy="false">}</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.Ex4.m1" class="ltx_Math" alttext="\displaystyle=\max\{3,\min\{2,x,y\},\ldots\}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mn>3</mn><mo>,</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mn>2</mn><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo stretchy="false">}</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.SSS1.p6" class="ltx_para ltx_noindent">
<p class="ltx_p">Obviously, the value of <math id="S4.SS1.SSS1.p6.m1" class="ltx_Math" alttext="\min\{2,x,y\}" display="inline"><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mn>2</mn><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">}</mo></mrow></mrow></math> can never be greater than 3. Therefore, we can prune the subtree rooted at <math id="S4.SS1.SSS1.p6.m2" class="ltx_Math" alttext="v_{2}" display="inline"><msub><mi>v</mi><mn>2</mn></msub></math>.</p>
</div>
<div id="S4.SS1.SSS1.p7" class="ltx_para ltx_noindent">
<p class="ltx_p">Based on this idea, we can define <math id="S4.SS1.SSS1.p7.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> as the best value that the MAX player has found so far at any choice point along the path for MAX, and <math id="S4.SS1.SSS1.p7.m2" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math> as the best value that the MIN player has found so far at any choice point along the path for MIN. Then, we can prune the subtree rooted at a node <math id="S4.SS1.SSS1.p7.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> if <math id="S4.SS1.SSS1.p7.m4" class="ltx_Math" alttext="\text{minimax}(n)\leq\alpha" display="inline"><mrow><mrow><mtext>minimax</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mi>α</mi></mrow></math> for MIN player, or <math id="S4.SS1.SSS1.p7.m5" class="ltx_Math" alttext="\text{minimax}(n)\geq\beta" display="inline"><mrow><mrow><mtext>minimax</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mi>β</mi></mrow></math> for MAX player.</p>
</div>
<div id="S4.SS1.SSS1.p8" class="ltx_para ltx_noindent">
<p class="ltx_p">Here is the pseudocode for the alpha-beta pruning algorithm:</p>
<pre class="ltx_verbatim ltx_font_typewriter"><code class="language-python">
def alphabeta_search(game, state):
  player = state.to_move

  def max_value(state, alpha, beta):
      if game.is_terminal(state):
          return game.utility(state, player), None
      v, move = -infinity, None
      for a in game.actions(state):
          v2, _ = min_value(game.result(state, a), alpha, beta)
          if v2 &gt; v:
              v, move = v2, a
          if v &gt;= beta:
              return v, move
          alpha = max(alpha, v)

      return v, move

  def min_value(state, alpha, beta):
      if game.is_terminal(state):
          return game.utility(state, player), None
      v, move = infinity, None
      for a in game.actions(state):
          v2, _ = max_value(game.result(state, a), alpha, beta)
          if v2 &lt; v:
              v, move = v2, a
          if v &lt;= alpha:
              return v, move
          beta = min(beta, v)

      return v, move

  return max_value(state, -infinity, +infinity)
</code></pre>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Move Ordering</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">From the process of alpha-beta pruning, we can see that the order of the children of a node is important. If we can find a good ordering, we can prune more nodes.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The ordering of the children of a node is often done by heuristic function, which evaluate the value of each child. For example, an Othello agent can use the difference in the number of pieces as the heuristic function.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Early Cutoffs</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Since utility function only provides scores for terminal states, the algorithm must reach the terminal states to evaluate, which is time-consuming. We can use early cutoffs to stop the search early. For example, we find an evaluation function that can estimate the value of a non-terminal state, thus we stop at any moment and use the stated examined so far to decide the best move.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">With this technique, we can use iterative deepening, where we gradually increase the depth of the search. This allows us to find a good move in a reasonable amount of time, or to stop the search early if we run out of time.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Stochastic Games</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Stochastic games may include randomness, for example in the form of dice rolls. In this case, we can use the <span class="ltx_text ltx_font_bold">expectiminimax algorithm</span>, which add a new type of node called <span class="ltx_text ltx_font_bold">chance node</span> to the game tree. The value of a chance node is the expected value of its children.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">For a state <math id="S4.SS2.p2.m1" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>, <math id="S4.SS2.p2.m2" class="ltx_Math" alttext="expectiminimax(s)" display="inline"><mrow><mi>e</mi><mo>⁢</mo><mi>x</mi><mo>⁢</mo><mi>p</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></math> is:</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<table id="S4.Ex5" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex5.m1" class="ltx_Math" alttext="\text{expectiminimax}(s)=\begin{cases}\text{utility}(s)&amp;\text{if }\text{terminal}(s)\\
\max_{a\in\text{actions}(s)}\text{expectiminimax}(\text{result}(s,a))&amp;\text{if%
 }\text{player}(s)=\text{MAX}\\
\min_{a\in\text{actions}(s)}\text{expectiminimax}(\text{result}(s,a))&amp;\text{if%
 }\text{player}(s)=\text{MIN}\\
\sum_{a\in\text{actions}(s)}\text{probability}(a)\times\text{expectiminimax}(%
\text{result}(s,a))&amp;\text{if }\text{player}(s)=\text{CHANCE}\end{cases}" display="block"><mrow><mrow><mtext>expectiminimax</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd columnalign="left"><mrow><mtext>utility</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mtext>if terminal</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mrow><msub><mi>max</mi><mrow><mi>a</mi><mo>∈</mo><mrow><mtext>actions</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo>⁡</mo><mtext>expectiminimax</mtext></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mtext>result</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mrow><mtext>if player</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mtext>MAX</mtext></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mrow><msub><mi>min</mi><mrow><mi>a</mi><mo>∈</mo><mrow><mtext>actions</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo>⁡</mo><mtext>expectiminimax</mtext></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mtext>result</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mrow><mtext>if player</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mtext>MIN</mtext></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mstyle displaystyle="false"><msub><mo largeop="true" symmetric="true">∑</mo><mrow><mi>a</mi><mo>∈</mo><mrow><mtext>actions</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub></mstyle><mrow><mrow><mrow><mtext>probability</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo>×</mo><mtext>expectiminimax</mtext></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mtext>result</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mtd><mtd columnalign="left"><mrow><mrow><mtext>if player</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mtext>CHANCE</mtext></mrow></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Constraint Satisfaction Problems</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For a constraint satisfaction problem (CSP), each state is defined by a set of variables <math id="S5.p1.m1" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> (factored representation), each of which has a domain of possible values <math id="S5.p1.m2" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math>, and a set of constraints <math id="S5.p1.m3" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> that specify allowable combinations of values for subsets of variables (unary, binary, global, and soft constraints).</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The goal of a CSP is to find an assignment of values to variables that satisfies all constraints. A solution to a CSP is called a <span class="ltx_text ltx_font_bold">consistent assignment</span>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Backtracking Search</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Backtracking search</span> is a variant of depth-first search that is used to solve CSPs. The major difference is that backtracking search can use inference to reduce the search space.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The algorithm works as follows:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Select an unassigned variable.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Select a value from the domain of the variable, and assign it to the variable.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the assignment is consistent with the constraints, recursively assign values to the remaining variables. Otherwise, backtrack.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">Several techniques can be used to improve the performance of backtracking search:</p>
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Minimum remaining values (MRV)</span>: When selecting an unassigned variable, choose the variable with the fewest remaining values in its domain.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Least constraining value (LCV)</span>: When selecting a value for a variable, choose the value that rules out the fewest values in the remaining variables.</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Forward checking</span>: When assigning a value to a variable, check if the remaining variables have any values left in their domains. If not, backtrack.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Types of Consistency</h3>

<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Node consistency</h5>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A variable is node consistent if every value in its domain satisfies the variable’s unary constraints.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Arc consistency</h5>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><math id="S5.SS2.SSS0.Px2.p1.m1" class="ltx_Math" alttext="X\to Y" display="inline"><mrow><mi>X</mi><mo>→</mo><mi>Y</mi></mrow></math> is arc consistent if for every value <math id="S5.SS2.SSS0.Px2.p1.m2" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> in the domain of <math id="S5.SS2.SSS0.Px2.p1.m3" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>, there is some value <math id="S5.SS2.SSS0.Px2.p1.m4" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> in the domain of <math id="S5.SS2.SSS0.Px2.p1.m5" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math> that satisfies the binary constraint between <math id="S5.SS2.SSS0.Px2.p1.m6" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> and <math id="S5.SS2.SSS0.Px2.p1.m7" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math>.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Path consistency</h5>

<div id="S5.SS2.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Generalize arc-consistency for multiple constraints. This is not important because it is always possible to transform all global constraints into binary constraints.</p>
</div>
</section>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>AC-3 Algorithm</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">AC-3 algorithm</span> checks whether a CSP is arc consistent. The algorithm works as follows:</p>
</div>
<figure id="algorithm1" class="ltx_float">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div class="ltx_listingline"> <span class="ltx_text ltx_font_bold">Input:</span> A CSP <math id="algorithm1.m1" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>, a set of variables <math id="algorithm1.m2" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math>, and a set of constraints <math id="algorithm1.m3" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math>
</div>
<div class="ltx_listingline"> <span class="ltx_text ltx_font_bold">Output:</span> Whether the CSP is arc consistent
</div>
<div class="ltx_listingline"> Initialize a queue <math id="algorithm1.m4" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> with all arcs in <math id="algorithm1.m5" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math>;
</div>
<div class="ltx_listingline"> <span class="ltx_text ltx_font_bold">while</span> <em class="ltx_emph ltx_font_italic"><math id="algorithm1.m6" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> is not empty</em> <span class="ltx_text ltx_font_bold">do</span> 
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    Remove the first arc <math id="algorithm1.m7" class="ltx_Math" alttext="(X_{i},X_{j})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math> from <math id="algorithm1.m8" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math>;
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_bold">if</span> <em class="ltx_emph ltx_font_italic">REVISE<math id="algorithm1.m9" class="ltx_Math" alttext="(X_{i},X_{j})" display="inline"><mrow><mo mathvariant="normal" stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo mathvariant="normal">,</mo><msub><mi>X</mi><mi>j</mi></msub><mo mathvariant="normal" stretchy="false">)</mo></mrow></math></em> <span class="ltx_text ltx_font_bold">then</span> 
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_bold">if</span> <em class="ltx_emph ltx_font_italic"><math id="algorithm1.m10" class="ltx_Math" alttext="D_{i}=\emptyset" display="inline"><mrow><msub><mi>D</mi><mi>i</mi></msub><mo mathvariant="normal">=</mo><mi mathvariant="normal">∅</mi></mrow></math></em> <span class="ltx_text ltx_font_bold">then</span> 
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_bold">return</span> <span class="ltx_text ltx_font_bold">false</span>;
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end if
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span class="ltx_text ltx_font_bold">for</span> <em class="ltx_emph ltx_font_italic">each <math id="algorithm1.m11" class="ltx_Math" alttext="X_{k}" display="inline"><msub><mi>X</mi><mi>k</mi></msub></math> such that <math id="algorithm1.m12" class="ltx_Math" alttext="X_{k}" display="inline"><msub><mi>X</mi><mi>k</mi></msub></math> is a neighbor of <math id="algorithm1.m13" class="ltx_Math" alttext="X_{i}" display="inline"><msub><mi>X</mi><mi>i</mi></msub></math> and <math id="algorithm1.m14" class="ltx_Math" alttext="X_{k}\neq X_{j}" display="inline"><mrow><msub><mi>X</mi><mi>k</mi></msub><mo mathvariant="normal">≠</mo><msub><mi>X</mi><mi>j</mi></msub></mrow></math></em> <span class="ltx_text ltx_font_bold">do</span> 
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    Add <math id="algorithm1.m15" class="ltx_Math" alttext="(X_{k},X_{i})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math> to <math id="algorithm1.m16" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math>;
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end for
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end if
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div class="ltx_listingline"> end while
</div>
<div class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span>AC-3 algorithm</figcaption>
</figure>
<figure id="algorithm2" class="ltx_float">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div class="ltx_listingline"> <span class="ltx_text ltx_font_bold">Input:</span> Two variables <math id="algorithm2.m3" class="ltx_Math" alttext="X_{i}" display="inline"><msub><mi>X</mi><mi>i</mi></msub></math> and <math id="algorithm2.m4" class="ltx_Math" alttext="X_{j}" display="inline"><msub><mi>X</mi><mi>j</mi></msub></math>
</div>
<div class="ltx_listingline"> <span class="ltx_text ltx_font_bold">Output:</span> Whether the domain of <math id="algorithm2.m5" class="ltx_Math" alttext="X_{i}" display="inline"><msub><mi>X</mi><mi>i</mi></msub></math> is revised
</div>
<div class="ltx_listingline"> <math id="algorithm2.m6" class="ltx_Math" alttext="revised\leftarrow\textbf{false}" display="inline"><mrow><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>v</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>d</mi></mrow><mo>←</mo><mtext>𝐟𝐚𝐥𝐬𝐞</mtext></mrow></math>;
</div>
<div class="ltx_listingline"> <span class="ltx_text ltx_font_bold">for</span> <em class="ltx_emph ltx_font_italic">each value <math id="algorithm2.m7" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> in <math id="algorithm2.m8" class="ltx_Math" alttext="D_{i}" display="inline"><msub><mi>D</mi><mi>i</mi></msub></math></em> <span class="ltx_text ltx_font_bold">do</span> 
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_bold">if</span> <em class="ltx_emph ltx_font_italic">no value <math id="algorithm2.m9" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> in <math id="algorithm2.m10" class="ltx_Math" alttext="D_{j}" display="inline"><msub><mi>D</mi><mi>j</mi></msub></math> allows <math id="algorithm2.m11" class="ltx_Math" alttext="(x,y)" display="inline"><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi>x</mi><mo mathvariant="normal">,</mo><mi>y</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></math> to satisfy the constraint between <math id="algorithm2.m12" class="ltx_Math" alttext="X_{i}" display="inline"><msub><mi>X</mi><mi>i</mi></msub></math> and <math id="algorithm2.m13" class="ltx_Math" alttext="X_{j}" display="inline"><msub><mi>X</mi><mi>j</mi></msub></math></em> <span class="ltx_text ltx_font_bold">then</span> 
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    Remove <math id="algorithm2.m14" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> from <math id="algorithm2.m15" class="ltx_Math" alttext="D_{i}" display="inline"><msub><mi>D</mi><mi>i</mi></msub></math>;
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <math id="algorithm2.m16" class="ltx_Math" alttext="revised\leftarrow\textbf{true}" display="inline"><mrow><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>v</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>d</mi></mrow><mo>←</mo><mtext>𝐭𝐫𝐮𝐞</mtext></mrow></math>;
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end if
</div>
<div class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div class="ltx_listingline"> end for
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">return</span> <math id="algorithm2.m17" class="ltx_Math" alttext="revised" display="inline"><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>v</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>d</mi></mrow></math>;
</div>
<div class="ltx_listingline"> 
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 2</span> </span>REVISE<math id="algorithm2.m2" class="ltx_Math" alttext="(X_{i},X_{j})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math></figcaption>
</figure>
<div id="S5.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The idea of the AC-3 algorithm is that:</p>
<ol id="S5.I3" class="ltx_enumerate">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I3.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Build a queue for arcs that need to be checked.</p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I3.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Check each arc in the queue. Modify the domain of the first variable until it is arc consistent with the second variable.</p>
</div>
</li>
<li id="S5.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I3.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the domain of the first variable is empty, the CSP is not arc consistent (no valid assignment for the first variable).</p>
</div>
</li>
<li id="S5.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I3.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Since we modified the domain of the first variable, this may cause some arcs to be inconsistent. Add these arcs to the queue.
</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">The time complexity of the AC-3 algorithm is <math id="S5.SS2.SSS1.p3.m1" class="ltx_Math" alttext="O(n^{2}d^{3})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>⁢</mo><msup><mi>d</mi><mn>3</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, where <math id="S5.SS2.SSS1.p3.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> is the number of variables and <math id="S5.SS2.SSS1.p3.m3" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> is the maximum domain size.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Logic</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Entailment and Inference</h3>

<section id="S6.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Entailment</h5>

<div id="S6.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A sentence <math id="S6.SS1.SSS0.Px1.p1.m1" class="ltx_Math" alttext="KB\models\alpha" display="inline"><mrow><mrow><mi>K</mi><mo>⁢</mo><mi>B</mi></mrow><mo>⊧</mo><mi>α</mi></mrow></math> means that <math id="S6.SS1.SSS0.Px1.p1.m2" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> is true in all models where <math id="S6.SS1.SSS0.Px1.p1.m3" class="ltx_Math" alttext="KB" display="inline"><mrow><mi>K</mi><mo>⁢</mo><mi>B</mi></mrow></math> is true. In other words, <math id="S6.SS1.SSS0.Px1.p1.m4" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> is entailed by <math id="S6.SS1.SSS0.Px1.p1.m5" class="ltx_Math" alttext="KB" display="inline"><mrow><mi>K</mi><mo>⁢</mo><mi>B</mi></mrow></math>.</p>
</div>
</section>
<section id="S6.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Inference</h5>

<div id="S6.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Inference is the process of deriving new sentences from existing sentences. It applies rules of inference to <math id="S6.SS1.SSS0.Px2.p1.m1" class="ltx_Math" alttext="KB" display="inline"><mrow><mi>K</mi><mo>⁢</mo><mi>B</mi></mrow></math> to build a new sentence <math id="S6.SS1.SSS0.Px2.p1.m2" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math>. Inference is denoted as <math id="S6.SS1.SSS0.Px2.p1.m3" class="ltx_Math" alttext="KB\vdash\alpha" display="inline"><mrow><mrow><mi>K</mi><mo>⁢</mo><mi>B</mi></mrow><mo>⊢</mo><mi>α</mi></mrow></math>.</p>
</div>
<div id="S6.SS1.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Entailment enumerates all possible models of <math id="S6.SS1.SSS0.Px2.p2.m1" class="ltx_Math" alttext="KB" display="inline"><mrow><mi>K</mi><mo>⁢</mo><mi>B</mi></mrow></math> to check if <math id="S6.SS1.SSS0.Px2.p2.m2" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> is true in all models, while inference does not.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Sound and Complete</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">An inference algorithm we desire should be sound and complete. A sound algorithm is one that never infer a false sentence from a true sentence, while a complete algorithm is one that can infer all true sentences. In other words, a sound and complete algorithm is one that can infer all and only true sentences.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">With inference rules, we can guarantee the soundness of algorithm. To achieve completeness, we use resolution or forward/backward chaining.</p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Resolution</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The resolution law can be written as:</p>
<table id="S6.Ex6" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.Ex6.m1" class="ltx_Math" alttext="\frac{p_{1}\lor p_{2}\lor\ldots\lor p_{n},\neg q_{1}\lor\neg q_{2}\lor\ldots%
\lor\neg q_{m}}{p_{1}\lor p_{2}\lor\ldots\lor p_{i-1}\lor p_{i+1}\lor\ldots%
\lor p_{n}\lor q_{1}\lor q_{2}\lor\ldots\lor q_{j-1}\lor q_{j+1}\lor\ldots\lor
q%
_{m}}" display="block"><mfrac><mrow><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>∨</mo><msub><mi>p</mi><mn>2</mn></msub><mo>∨</mo><mi mathvariant="normal">…</mi><mo>∨</mo><msub><mi>p</mi><mi>n</mi></msub></mrow><mo>,</mo><mrow><mrow><mi mathvariant="normal">¬</mi><mo>⁢</mo><msub><mi>q</mi><mn>1</mn></msub></mrow><mo>∨</mo><mrow><mi mathvariant="normal">¬</mi><mo>⁢</mo><msub><mi>q</mi><mn>2</mn></msub></mrow><mo>∨</mo><mi mathvariant="normal">…</mi><mo>∨</mo><mrow><mi mathvariant="normal">¬</mi><mo>⁢</mo><msub><mi>q</mi><mi>m</mi></msub></mrow></mrow></mrow><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>∨</mo><msub><mi>p</mi><mn>2</mn></msub><mo>∨</mo><mi mathvariant="normal">…</mi><mo>∨</mo><msub><mi>p</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>∨</mo><msub><mi>p</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∨</mo><mi mathvariant="normal">…</mi><mo>∨</mo><msub><mi>p</mi><mi>n</mi></msub><mo>∨</mo><msub><mi>q</mi><mn>1</mn></msub><mo>∨</mo><msub><mi>q</mi><mn>2</mn></msub><mo>∨</mo><mi mathvariant="normal">…</mi><mo>∨</mo><msub><mi>q</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>∨</mo><msub><mi>q</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∨</mo><mi mathvariant="normal">…</mi><mo>∨</mo><msub><mi>q</mi><mi>m</mi></msub></mrow></mfrac></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S6.SS2.SSS1.p1.m1" class="ltx_Math" alttext="p_{i}" display="inline"><msub><mi>p</mi><mi>i</mi></msub></math> and <math id="S6.SS2.SSS1.p1.m2" class="ltx_Math" alttext="q_{j}" display="inline"><msub><mi>q</mi><mi>j</mi></msub></math> are complementary literals.</p>
</div>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Forward and Backward Chaining</h4>

<section id="S6.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Forward Chaining</h5>

<div id="S6.SS2.SSS2.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Forward chaining is a simple inference algorithm that starts with the known facts and repeatedly applies Modus Ponens to Horn Clauses and add result back to <math id="S6.SS2.SSS2.Px1.p1.m1" class="ltx_Math" alttext="KB" display="inline"><mrow><mi>K</mi><mo>⁢</mo><mi>B</mi></mrow></math>.</p>
</div>
</section>
<section id="S6.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Backward Chaining</h5>

<div id="S6.SS2.SSS2.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Backward chaining is goal-driven. It repeatedly checks the premise of target clause. This gives linear complexity in size of <math id="S6.SS2.SSS2.Px2.p1.m1" class="ltx_Math" alttext="KB" display="inline"><mrow><mi>K</mi><mo>⁢</mo><mi>B</mi></mrow></math>.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Machine Learning</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Supervised vs. Unsupervised Learning</h3>

<section id="S7.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Supervised Learning</h5>

<div id="S7.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In supervised learning, the algorithm learns from labeled data, where each example is a pair of input and output. The goal is to learn a function that maps inputs to outputs. Within supervised learning, we have two types of tasks: <span class="ltx_text ltx_font_bold">classification</span> (discrete output) and <span class="ltx_text ltx_font_bold">regression</span> (continuous output).</p>
</div>
</section>
<section id="S7.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Unsupervised Learning</h5>

<div id="S7.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In unsupervised learning, the algorithm learns from unlabeled data. The goal is to find hidden patterns in the data.</p>
</div>
</section>
<section id="S7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1 </span>Example: K-Nearest Neighbors</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The k-nearest neighbors (KNN) is a simple supervised learning algorithm that can be used for both classification and regression. Although most ML algorithms build a model from the training data, KNN algorithm is an exception.</p>
</div>
<div id="S7.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The core idea of the KNN algorithm that two data points are similar if they are close to each other, and two similar data points should have the same label. The algorithm works as follows:</p>
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Train</span> Add all data points to the training set.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Predict</span>
</p>
<ol id="S7.I1.i2.I1" class="ltx_enumerate">
<li id="S7.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I1.i2.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For a new data point, calculate the distance between the new data point and all data points in the training set. For this step, we can use Euclidean distance, Manhattan distance, etc.</p>
</div>
</li>
<li id="S7.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I1.i2.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Select the <math id="S7.I1.i2.I1.i2.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> nearest data points.</p>
<ul id="S7.I1.i2.I1.i2.I1" class="ltx_itemize">
<li id="S7.I1.i2.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I1.i2.I1.i2.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For classification, return the most common label among the <math id="S7.I1.i2.I1.i2.I1.i1.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> nearest data points.</p>
</div>
</li>
<li id="S7.I1.i2.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I1.i2.I1.i2.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For regression, return the average value of the <math id="S7.I1.i2.I1.i2.I1.i2.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> nearest data points.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div id="S7.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">The pros and cons of KNN are:</p>
<ul id="S7.I2" class="ltx_itemize">
<li id="S7.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Pros</p>
<ul id="S7.I2.i1.I1" class="ltx_itemize">
<li id="S7.I2.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I2.i1.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Simple</p>
</div>
</li>
<li id="S7.I2.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I2.i1.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">No training phase</p>
</div>
</li>
<li id="S7.I2.i1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I2.i1.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">No model or hyperparameters, no assumptions about the data</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S7.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Cons</p>
<ul id="S7.I2.i2.I1" class="ltx_itemize">
<li id="S7.I2.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I2.i2.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Slow prediction phase (<math id="S7.I2.i2.I1.i1.p1.m1" class="ltx_Math" alttext="O(nd)" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, where <math id="S7.I2.i2.I1.i1.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> is the number of data points and <math id="S7.I2.i2.I1.i1.p1.m3" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> is the number of features)</p>
</div>
</li>
<li id="S7.I2.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I2.i2.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Require large dataset</p>
</div>
</li>
<li id="S7.I2.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I2.i2.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Sensitive to the distance metric</p>
</div>
</li>
<li id="S7.I2.i2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span> 
<div id="S7.I2.i2.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Suffers from the curse of dimensionality</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Train, Validate and Test</h3>

<section id="S7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1 </span>Loss Function</h4>

<div id="S7.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">To evaluate the performance of a model, we need a loss function that measures the difference between the predicted value and the true value. Different tasks require different loss functions.</p>
</div>
<div id="S7.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">For classification tasks, we often use the cross-entropy loss, which is defined as:</p>
<table id="S7.Ex7" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.Ex7.m1" class="ltx_Math" alttext="\text{Cross-entropy loss}=-\sum_{i=1}^{n}y_{i}\log(p_{i})" display="block"><mrow><mtext>Cross-entropy loss</mtext><mo>=</mo><mrow><mo>-</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S7.SS2.SSS1.p2.m1" class="ltx_Math" alttext="y_{i}" display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the true label in one-hot encoding, and <math id="S7.SS2.SSS1.p2.m2" class="ltx_Math" alttext="p_{i}" display="inline"><msub><mi>p</mi><mi>i</mi></msub></math> is the predicted probability.</p>
</div>
<div id="S7.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">For regression tasks, we often use the mean squared error (MSE), which is defined as:</p>
<table id="S7.Ex8" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.Ex8.m1" class="ltx_Math" alttext="\text{MSE}=\frac{1}{2n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}" display="block"><mrow><mtext>MSE</mtext><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>⁢</mo><mi>n</mi></mrow></mfrac><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><msub><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</section>
<section id="S7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2 </span>Structural Risk Minimization</h4>

<figure id="S7.F6" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig7.png" id="S7.F6.g1" class="ltx_graphics ltx_centering" width="325" height="287" alt="Overfitting and underfitting.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Overfitting and underfitting.</figcaption>
</figure>
<div id="S7.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">When we train a model, we want to find a balance between underfitting and overfitting. Underfitting occurs when the model is too simple to capture the underlying pattern in the data, while overfitting occurs when the model is too complex and captures noise in the data.</p>
</div>
<div id="S7.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Underfitting is featured by high bias and low variance, with high training error and high test error. Overfitting is featured by low bias and high variance, with low training error and high test error.</p>
</div>
<div id="S7.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">To avoid overfitting, here are some techniques:</p>
<ul id="S7.I3" class="ltx_itemize">
<li id="S7.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I3.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Feature selection</span>: Remove irrelevant features.</p>
</div>
</li>
<li id="S7.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I3.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Regularization</span>: Add a penalty term to the loss function.</p>
</div>
</li>
<li id="S7.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I3.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Cross-validation</span>: Estimate the test error with a validation set.</p>
</div>
</li>
<li id="S7.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I3.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Model selection</span>: Choose a simpler model (lower degree polynomial, smaller neural network, etc.).</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S7.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.3 </span>K-Fold Cross-Validation</h4>

<div id="S7.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">K-fold cross-validation is a technique that estimates the test error by splitting the training set into <math id="S7.SS2.SSS3.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> folds. The algorithm works as follows:</p>
<ol id="S7.I4" class="ltx_enumerate">
<li id="S7.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I4.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Randomly partition the training set <math id="S7.I4.i1.p1.m1" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> into <math id="S7.I4.i1.p1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> equal-sized subsets, denoted as <math id="S7.I4.i1.p1.m3" class="ltx_Math" alttext="D_{1},D_{2},\ldots,D_{k}" display="inline"><mrow><msub><mi>D</mi><mn>1</mn></msub><mo>,</mo><msub><mi>D</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>D</mi><mi>k</mi></msub></mrow></math>.</p>
</div>
</li>
<li id="S7.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I4.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For <math id="S7.I4.i2.p1.m1" class="ltx_Math" alttext="i=1,2,\ldots,k" display="inline"><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>k</mi></mrow></mrow></math>, train the model on <math id="S7.I4.i2.p1.m2" class="ltx_Math" alttext="D-D_{i}" display="inline"><mrow><mi>D</mi><mo>-</mo><msub><mi>D</mi><mi>i</mi></msub></mrow></math> and evaluate the model on <math id="S7.I4.i2.p1.m3" class="ltx_Math" alttext="D_{i}" display="inline"><msub><mi>D</mi><mi>i</mi></msub></math>.</p>
</div>
</li>
<li id="S7.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S7.I4.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Calculate the average test error.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S7.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.4 </span>Confusion Matrix</h4>

<div id="S7.SS2.SSS4.p1" class="ltx_para">
<p class="ltx_p">A confusion matrix is a table that is often used to describe the performance of a classification model. An example of a confusion matrix is shown below:</p>
</div>
<figure id="S7.SS2.SSS4.tab1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Predicted Positive</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Predicted Negative</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span class="ltx_text ltx_font_bold">Actual Positive</span></th>
<td class="ltx_td ltx_align_center ltx_border_t">True Positive (TP)</td>
<td class="ltx_td ltx_align_center ltx_border_t">False Negative (FN)</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text ltx_font_bold">Actual Negative</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb">False Positive (FP)</td>
<td class="ltx_td ltx_align_center ltx_border_bb">True Negative (TN)</td>
</tr>
</tbody>
</table>
</figure>
<div id="S7.SS2.SSS4.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Some common metrics that can be calculated from a confusion matrix include:</p>
<ul id="S7.I5" class="ltx_itemize">
<li id="S7.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I5.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Accuracy</span>: <math id="S7.I5.i1.p1.m1" class="ltx_Math" alttext="\frac{TP+TN}{TP+FP+FN+TN}" display="inline"><mfrac><mrow><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mo>+</mo><mrow><mi>T</mi><mo>⁢</mo><mi>N</mi></mrow></mrow><mrow><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mo>+</mo><mrow><mi>F</mi><mo>⁢</mo><mi>P</mi></mrow><mo>+</mo><mrow><mi>F</mi><mo>⁢</mo><mi>N</mi></mrow><mo>+</mo><mrow><mi>T</mi><mo>⁢</mo><mi>N</mi></mrow></mrow></mfrac></math>, the proportion of correct predictions.</p>
</div>
</li>
<li id="S7.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I5.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Precision</span>: <math id="S7.I5.i2.p1.m1" class="ltx_Math" alttext="\frac{TP}{TP+FP}" display="inline"><mfrac><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mrow><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mo>+</mo><mrow><mi>F</mi><mo>⁢</mo><mi>P</mi></mrow></mrow></mfrac></math>, the proportion of positive predictions that are correct.</p>
</div>
</li>
<li id="S7.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I5.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Recall</span>: <math id="S7.I5.i3.p1.m1" class="ltx_Math" alttext="\frac{TP}{TP+FN}" display="inline"><mfrac><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mrow><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mo>+</mo><mrow><mi>F</mi><mo>⁢</mo><mi>N</mi></mrow></mrow></mfrac></math>, the proportion of actual positives that are correctly predicted.</p>
</div>
</li>
<li id="S7.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I5.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">F1 score</span>: <math id="S7.I5.i4.p1.m1" class="ltx_Math" alttext="2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}" display="inline"><mrow><mn>2</mn><mo>×</mo><mfrac><mrow><mtext>Precision</mtext><mo>×</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></mrow></math>, the harmonic mean of precision and recall.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Regression</h2>

<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Linear Regression</h3>

<div id="S8.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A linear regression model is a linear model (i.e., the output is a linear combination of the input features) that is used to predict a continuous value. The model is defined as:</p>
<table id="S8.Ex9" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S8.Ex9.m1" class="ltx_Math" alttext="y=w_{0}+\sum_{i=1}^{d}w_{i}x_{i}" display="block"><mrow><mi>y</mi><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S8.SS1.p1.m1" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> is the predicted value, <math id="S8.SS1.p1.m2" class="ltx_Math" alttext="w_{0}" display="inline"><msub><mi>w</mi><mn>0</mn></msub></math> is the bias term, <math id="S8.SS1.p1.m3" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> are the weights, <math id="S8.SS1.p1.m4" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> are the input features, and <math id="S8.SS1.p1.m5" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> is the dimension of the input features.</p>
</div>
<div id="S8.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The goal of linear regression is to find the weights that minimize the mean squared error (MSE) between the predicted value and the true value. The loss function is defined as:</p>
<table id="S8.Ex10" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S8.Ex10.m1" class="ltx_Math" alttext="\text{MSE}=\frac{1}{2n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}" display="block"><mrow><mtext>MSE</mtext><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>⁢</mo><mi>n</mi></mrow></mfrac><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><msub><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S8.SS1.p2.m1" class="ltx_Math" alttext="y_{i}" display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the true value, <math id="S8.SS1.p2.m2" class="ltx_Math" alttext="\hat{y}_{i}" display="inline"><msub><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mi>i</mi></msub></math> is the predicted value, and <math id="S8.SS1.p2.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> is the number of data points.</p>
</div>
<section id="S8.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.1 </span>Normal Equation</h4>

<div id="S8.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Let <math id="S8.SS1.SSS1.p1.m1" class="ltx_Math" alttext="\mathbf{X}=\begin{bmatrix}1&amp;x_{11}&amp;x_{12}&amp;\ldots&amp;x_{1d}\\
1&amp;x_{21}&amp;x_{22}&amp;\ldots&amp;x_{2d}\\
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
1&amp;x_{n1}&amp;x_{n2}&amp;\ldots&amp;x_{nd}\end{bmatrix}" display="inline"><mrow><mi>𝐗</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><msub><mi>x</mi><mn>11</mn></msub></mtd><mtd columnalign="center"><msub><mi>x</mi><mn>12</mn></msub></mtd><mtd columnalign="center"><mi mathvariant="normal">…</mi></mtd><mtd columnalign="center"><msub><mi>x</mi><mrow><mn>1</mn><mo>⁢</mo><mi>d</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><msub><mi>x</mi><mn>21</mn></msub></mtd><mtd columnalign="center"><msub><mi>x</mi><mn>22</mn></msub></mtd><mtd columnalign="center"><mi mathvariant="normal">…</mi></mtd><mtd columnalign="center"><msub><mi>x</mi><mrow><mn>2</mn><mo>⁢</mo><mi>d</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center"><mi mathvariant="normal">⋮</mi></mtd><mtd columnalign="center"><mi mathvariant="normal">⋮</mi></mtd><mtd columnalign="center"><mi mathvariant="normal">⋮</mi></mtd><mtd columnalign="center"><mi mathvariant="normal">⋱</mi></mtd><mtd columnalign="center"><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><msub><mi>x</mi><mrow><mi>n</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center"><msub><mi>x</mi><mrow><mi>n</mi><mo>⁢</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center"><mi mathvariant="normal">…</mi></mtd><mtd columnalign="center"><msub><mi>x</mi><mrow><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>, <math id="S8.SS1.SSS1.p1.m2" class="ltx_Math" alttext="\mathbf{y}=\begin{bmatrix}y_{1}\\
y_{2}\\
\vdots\\
y_{n}\end{bmatrix}" display="inline"><mrow><mi>𝐲</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd columnalign="center"><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>y</mi><mi>n</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>, and <math id="S8.SS1.SSS1.p1.m3" class="ltx_Math" alttext="\mathbf{w}=\begin{bmatrix}w_{0}\\
w_{1}\\
w_{2}\\
\vdots\\
w_{d}\end{bmatrix}" display="inline"><mrow><mi>𝐰</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd columnalign="center"><msub><mi>w</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>w</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>w</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>w</mi><mi>d</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>.</p>
</div>
<div id="S8.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Then the loss function can be written as:</p>
<table id="S8.Ex11" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S8.Ex11.m1" class="ltx_Math" alttext="\text{MSE}=\frac{1}{2n}(\mathbf{y}-\mathbf{X}\mathbf{w})^{T}(\mathbf{y}-%
\mathbf{X}\mathbf{w})" display="block"><mrow><mtext>MSE</mtext><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>⁢</mo><mi>n</mi></mrow></mfrac><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝐲</mi><mo>-</mo><mi>𝐗𝐰</mi></mrow><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝐲</mi><mo>-</mo><mi>𝐗𝐰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S8.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">The unique solution to the normal equation <math id="S8.SS1.SSS1.p3.m1" class="ltx_Math" alttext="\frac{\partial\text{MSE}}{\partial\mathbf{w}}=0" display="inline"><mrow><mfrac><mrow><mo>∂</mo><mo>⁡</mo><mtext>MSE</mtext></mrow><mrow><mo>∂</mo><mo>⁡</mo><mi>𝐰</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math> is:</p>
<table id="S8.Ex12" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S8.Ex12.m1" class="ltx_Math" alttext="\mathbf{w}=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}" display="block"><mrow><mi>𝐰</mi><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝐗</mi><mi>T</mi></msup><mo>⁢</mo><mi>𝐗</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><msup><mi>𝐗</mi><mi>T</mi></msup><mo>⁢</mo><mi>𝐲</mi></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S8.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">However, the normal equation has some limitations:</p>
<ul id="S8.I1" class="ltx_itemize">
<li id="S8.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">We cannot guarantee that <math id="S8.I1.i1.p1.m1" class="ltx_Math" alttext="(\mathbf{X}^{T}\mathbf{X})^{-1}" display="inline"><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝐗</mi><mi>T</mi></msup><mo>⁢</mo><mi>𝐗</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></math> exists. In some extreme cases (for example, when <math id="S8.I1.i1.p1.m2" class="ltx_Math" alttext="d&gt;n" display="inline"><mrow><mi>d</mi><mo>&gt;</mo><mi>n</mi></mrow></math>), we can prove that <math id="S8.I1.i1.p1.m3" class="ltx_Math" alttext="(\mathbf{X}^{T}\mathbf{X})^{-1}" display="inline"><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝐗</mi><mi>T</mi></msup><mo>⁢</mo><mi>𝐗</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></math> does not exist.</p>
</div>
</li>
<li id="S8.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Even if <math id="S8.I1.i2.p1.m1" class="ltx_Math" alttext="(\mathbf{X}^{T}\mathbf{X})^{-1}" display="inline"><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝐗</mi><mi>T</mi></msup><mo>⁢</mo><mi>𝐗</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></math> exists, the time complexity for matrix inversion is <math id="S8.I1.i2.p1.m2" class="ltx_Math" alttext="O(d^{3})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>d</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow></mrow></math>, which is not efficient for large <math id="S8.I1.i2.p1.m3" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math>.</p>
</div>
</li>
<li id="S8.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Due to its linear nature, the normal equation cannot capture non-linear relationships between the input features and the output.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S8.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.2 </span>Gradient Descent</h4>

<div id="S8.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Gradient descent is an iterative optimization algorithm that is used to find the weights that minimize the loss function. The algorithm works as follows:</p>
<ol id="S8.I2" class="ltx_enumerate">
<li id="S8.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S8.I2.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Initialize the weights <math id="S8.I2.i1.p1.m1" class="ltx_Math" alttext="\mathbf{w}" display="inline"><mi>𝐰</mi></math> randomly.</p>
</div>
</li>
<li id="S8.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S8.I2.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Calculate the gradient of the loss function with respect to the weights: <math id="S8.I2.i2.p1.m1" class="ltx_Math" alttext="\frac{\partial\text{MSE}}{\partial\mathbf{w}}" display="inline"><mfrac><mrow><mo>∂</mo><mo>⁡</mo><mtext>MSE</mtext></mrow><mrow><mo>∂</mo><mo>⁡</mo><mi>𝐰</mi></mrow></mfrac></math>.</p>
</div>
</li>
<li id="S8.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S8.I2.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Update the weights: <math id="S8.I2.i3.p1.m1" class="ltx_Math" alttext="\mathbf{w}\leftarrow\mathbf{w}-\alpha\frac{\partial\text{MSE}}{\partial\mathbf%
{w}}" display="inline"><mrow><mi>𝐰</mi><mo>←</mo><mrow><mi>𝐰</mi><mo>-</mo><mrow><mi>α</mi><mo>⁢</mo><mfrac><mrow><mo>∂</mo><mo>⁡</mo><mtext>MSE</mtext></mrow><mrow><mo>∂</mo><mo>⁡</mo><mi>𝐰</mi></mrow></mfrac></mrow></mrow></mrow></math>, where <math id="S8.I2.i3.p1.m2" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> is the learning rate.</p>
</div>
</li>
<li id="S8.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S8.I2.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Repeat steps 2 and 3 until the weights converge.</p>
</div>
</li>
</ol>
</div>
<div id="S8.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">Example:</span>
</p>
</div>
<div id="S8.SS1.SSS2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">For a first order linear regression model defined as:</p>
<table id="S8.Ex13" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S8.Ex13.m1" class="ltx_Math" alttext="y=w_{0}+w_{1}x" display="block"><mrow><mi>y</mi><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><mi>x</mi></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">the update rule for gradient descent is:</p>
<table id="S14.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S8.Ex14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S8.Ex14.m1" class="ltx_Math" alttext="\displaystyle w_{0}" display="inline"><msub><mi>w</mi><mn>0</mn></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S8.Ex14.m2" class="ltx_Math" alttext="\displaystyle\leftarrow w_{0}-\alpha\frac{1}{n}\sum_{i=1}^{n}(w_{0}+w_{1}x_{i}%
-y_{i})" display="inline"><mrow><mi></mi><mo>←</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>-</mo><mrow><mi>α</mi><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow><mo>-</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S8.Ex15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S8.Ex15.m1" class="ltx_Math" alttext="\displaystyle w_{1}" display="inline"><msub><mi>w</mi><mn>1</mn></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S8.Ex15.m2" class="ltx_Math" alttext="\displaystyle\leftarrow w_{1}-\alpha\frac{1}{n}\sum_{i=1}^{n}(w_{0}+w_{1}x_{i}%
-y_{i})x_{i}" display="inline"><mrow><mi></mi><mo>←</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>-</mo><mrow><mi>α</mi><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow><mo>-</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Logistic Regression</h3>

<div id="S8.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Logistic regression is a linear model that is used to predict the probability of a binary outcome. The model is defined as:</p>
<table id="S8.Ex16" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S8.Ex16.m1" class="ltx_Math" alttext="y=\sigma(w_{0}+\sum_{i=1}^{d}w_{i}x_{i})" display="block"><mrow><mi>y</mi><mo>=</mo><mrow><mi>σ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S8.SS2.p1.m1" class="ltx_Math" alttext="\sigma(z)=\frac{1}{1+e^{-z}}" display="inline"><mrow><mrow><mi>σ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math> is the sigmoid function.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The loss function for logistic regression is the cross-entropy loss, which is defined as:</p>
<table id="S8.Ex17" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S8.Ex17.m1" class="ltx_Math" alttext="\text{Cross-entropy loss}=-\frac{1}{n}\sum_{i=1}^{n}y_{i}\log(p_{i})+(1-y_{i})%
\log(1-p_{i})" display="block"><mrow><mtext>Cross-entropy loss</mtext><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>p</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Support Vector Machine</h2>

<div id="S9.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A support vector machine (SVM) is a supervised learning algorithm that is used for classification tasks. The goal of an SVM is to find the hyperplane that separates the data points of different classes with the maximum margin.</p>
</div>
<section id="S9.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Linear SVM</h3>

<div id="S9.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">One challenge met by linear regression is that many hyperplanes can separate the data points, but which one is the best? SVM solves this problem. The core idea of linear SVM is to find a hyperplane that its minimum distance to any data point is maximized. Although the idea looks simple, the mathematical formulation of the problem is complex, however beautiful.</p>
</div>
<div id="S9.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">A linear SVM is a linear model that is defined as:</p>
<table id="S9.Ex18" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex18.m1" class="ltx_Math" alttext="y=\text{sign}\left(b+\sum_{i=1}^{d}w_{i}x_{i}\right)" display="block"><mrow><mi>y</mi><mo>=</mo><mrow><mtext>sign</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>b</mi><mo>+</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where sign is the sign function, defined as:</p>
<table id="S9.Ex19" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex19.m1" class="ltx_Math" alttext="\text{sign}(z)=\begin{cases}1&amp;\text{if }z\geq 0\\
-1&amp;\text{otherwise}\end{cases}" display="block"><mrow><mrow><mtext>sign</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd columnalign="left"><mn>1</mn></mtd><mtd columnalign="left"><mrow><mrow><mtext>if </mtext><mo>⁢</mo><mi>z</mi></mrow><mo>≥</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S9.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">The decision boundary of an SVM is defined as:</p>
<table id="S9.Ex20" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex20.m1" class="ltx_Math" alttext="b+\sum_{i=1}^{d}w_{i}x_{i}=0" display="block"><mrow><mrow><mi>b</mi><mo>+</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S9.SS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">Since we have a decision boundary, we can find the minimum distance <math id="S9.SS1.p4.m1" class="ltx_Math" alttext="c" display="inline"><mi>c</mi></math> between the decision boundary and the closest data point. This distance is called the margin. Now we define the plus-plane as <math id="S9.SS1.p4.m2" class="ltx_Math" alttext="b+\sum_{i=1}^{d}w_{i}x_{i}\geq c" display="inline"><mrow><mrow><mi>b</mi><mo>+</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>≥</mo><mi>c</mi></mrow></math> and the minus-plane as <math id="S9.SS1.p4.m3" class="ltx_Math" alttext="b+\sum_{i=1}^{d}w_{i}x_{i}\leq-c" display="inline"><mrow><mrow><mi>b</mi><mo>+</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>≤</mo><mrow><mo>-</mo><mi>c</mi></mrow></mrow></math>. For any point, its distance to the decision boundary is at least <math id="S9.SS1.p4.m4" class="ltx_Math" alttext="c" display="inline"><mi>c</mi></math>.</p>
</div>
<div id="S9.SS1.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">To simplify the problem, we can scale the weights and the bias term by <math id="S9.SS1.p5.m1" class="ltx_Math" alttext="c" display="inline"><mi>c</mi></math> (i.e., <math id="S9.SS1.p5.m2" class="ltx_Math" alttext="w_{i}\leftarrow\frac{w_{i}}{c}" display="inline"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>←</mo><mfrac><msub><mi>w</mi><mi>i</mi></msub><mi>c</mi></mfrac></mrow></math> and <math id="S9.SS1.p5.m3" class="ltx_Math" alttext="b\leftarrow\frac{b}{c}" display="inline"><mrow><mi>b</mi><mo>←</mo><mfrac><mi>b</mi><mi>c</mi></mfrac></mrow></math>). Now the plus-plane becomes <math id="S9.SS1.p5.m4" class="ltx_Math" alttext="b+\sum_{i=1}^{d}w_{i}x_{i}\geq 1" display="inline"><mrow><mrow><mi>b</mi><mo>+</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>≥</mo><mn>1</mn></mrow></math> and the minus-plane becomes <math id="S9.SS1.p5.m5" class="ltx_Math" alttext="b+\sum_{i=1}^{d}w_{i}x_{i}\leq-1" display="inline"><mrow><mrow><mi>b</mi><mo>+</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>≤</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow></math>. For any point, its distance to the decision boundary is at least 1.</p>
</div>
<div id="S9.SS1.p6" class="ltx_para ltx_noindent">
<p class="ltx_p">Finally, we can define the margin as the distance between the plus-plane and the minus-plane, which is <math id="S9.SS1.p6.m1" class="ltx_Math" alttext="\frac{2}{\|w\|}" display="inline"><mfrac><mn>2</mn><mrow><mo>∥</mo><mi>w</mi><mo>∥</mo></mrow></mfrac></math>. Any data point that is on the plus-plane or the minus-plane is called a support vector.
</p>
</div>
<figure id="S9.F7" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig8.png" id="S9.F7.g1" class="ltx_graphics ltx_centering" width="325" height="316" alt="The margin of an SVM.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The margin of an SVM.</figcaption>
</figure>
<div id="S9.SS1.p7" class="ltx_para ltx_noindent">
<p class="ltx_p">The goal of an SVM is to maximize the margin. This has a clear interpretation: the larger the margin, the more separation between the classes.</p>
</div>
<div id="S9.SS1.p8" class="ltx_para ltx_noindent">
<p class="ltx_p">Now, if we skip the derivation, we can find that the optimization problem of an SVM can be simply written as:</p>
<table id="S9.Ex21" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex21.m1" class="ltx_Math" alttext="\min_{w,b}\frac{1}{2}\|w\|^{2}\quad\text{while satisfying}\quad y_{i}(w^{T}x_{i}+b)\geq 1\quad\text{for all }i" display="block"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>w</mi><mo>,</mo><mi>b</mi></mrow></munder><mo>⁡</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><msup><mrow><mo>∥</mo><mi>w</mi><mo>∥</mo></mrow><mn>2</mn></msup></mrow></mrow><mo mathvariant="italic" separator="true"> </mo><mtext>while satisfying</mtext><mo mathvariant="italic" separator="true"> </mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>w</mi><mi>T</mi></msup><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>≥</mo><mn>1</mn></mrow><mo mathvariant="italic" separator="true"> </mo><mrow><mtext>for all </mtext><mo>⁢</mo><mi>i</mi></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S9.SS1.p8.m1" class="ltx_Math" alttext="y_{i}" display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the label of the <math id="S9.SS1.p8.m2" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>-th data point.</p>
</div>
<section id="S9.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.1 </span>Solving the Optimization Problem</h4>

<div id="S9.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The optimization problem of an SVM is a quadratic programming problem. We can use the Lagrange duality to solve it.</p>
</div>
<section id="S9.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Construct the Lagrangian</h5>

<div id="S9.SS1.SSS1.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The Lagrangian of the optimization problem is:</p>
<table id="S9.Ex22" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex22.m1" class="ltx_Math" alttext="L(w,b,\alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{n}\alpha_{i}(y_{i}(w^{T}x_{i}+b%
)-1)" display="block"><mrow><mrow><mi>L</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><msup><mrow><mo>∥</mo><mi>w</mi><mo>∥</mo></mrow><mn>2</mn></msup></mrow><mo>-</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>w</mi><mi>T</mi></msup><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S9.SS1.SSS1.Px1.p1.m1" class="ltx_Math" alttext="\alpha_{i}\geq 0" display="inline"><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>≥</mo><mn>0</mn></mrow></math> are the Lagrange multipliers.
</p>
</div>
</section>
<section id="S9.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Find the Dual Problem</h5>

<div id="S9.SS1.SSS1.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The dual problem is:</p>
<table id="S9.Ex23" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex23.m1" class="ltx_Math" alttext="\max_{\alpha}\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}%
\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}\quad\text{while satisfying}\quad%
\alpha_{i}\geq 0\quad\sum_{i=1}^{n}\alpha_{i}y_{i}=0" display="block"><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>α</mi></munder><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>α</mi><mi>i</mi></msub></mrow></mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>α</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>y</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>y</mi><mi>j</mi></msub><mo>⁢</mo><msubsup><mi>x</mi><mi>i</mi><mi>T</mi></msubsup><mo>⁢</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow></mrow><mo mathvariant="italic" separator="true"> </mo><mtext>while satisfying</mtext><mo mathvariant="italic" separator="true"> </mo><msub><mi>α</mi><mi>i</mi></msub></mrow><mo>≥</mo><mn>0</mn></mrow><mo mathvariant="italic" separator="true"> </mo><mrow><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S9.SS1.SSS1.Px2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Using any optimization algorithm, we can find the optimal <math id="S9.SS1.SSS1.Px2.p2.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math>.</p>
</div>
</section>
<section id="S9.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Find the Optimal Weights</h5>

<div id="S9.SS1.SSS1.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The optimal weights can be calculated as:</p>
<table id="S9.Ex24" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex24.m1" class="ltx_Math" alttext="w=\sum_{i=1}^{n}\alpha_{i}y_{i}x_{i}" display="block"><mrow><mi>w</mi><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>y</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</section>
<section id="S9.SS1.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Find the Optimal Bias Term</h5>

<div id="S9.SS1.SSS1.Px4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The optimal bias term can be calculated as:</p>
<table id="S9.Ex25" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex25.m1" class="ltx_Math" alttext="b=y_{i}-\sum_{j=1}^{n}\alpha_{j}y_{j}x_{j}^{T}x_{i}" display="block"><mrow><mi>b</mi><mo>=</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>α</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>y</mi><mi>j</mi></msub><mo>⁢</mo><msubsup><mi>x</mi><mi>j</mi><mi>T</mi></msubsup><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S9.SS1.SSS1.Px4.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">Example:</span>
</p>
</div>
<div id="S9.SS1.SSS1.Px4.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">KKT complementarity conditions tell us that, after we find the optimal <math id="S9.SS1.SSS1.Px4.p3.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math>, all support vectors have <math id="S9.SS1.SSS1.Px4.p3.m2" class="ltx_Math" alttext="\alpha_{i}&gt;0" display="inline"><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>&gt;</mo><mn>0</mn></mrow></math> and all other data points have <math id="S9.SS1.SSS1.Px4.p3.m3" class="ltx_Math" alttext="\alpha_{i}=0" display="inline"><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow></math>.</p>
</div>
</section>
</section>
<section id="S9.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.2 </span>Soft Margin SVM</h4>

<div id="S9.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Sometimes, the data points are not linearly separable. In this case, we can use a soft margin SVM, which allows some data points to be misclassified. The optimization problem of a soft margin SVM is:</p>
<table id="S9.Ex26" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex26.m1" class="ltx_Math" alttext="\min_{w,b,\xi}\frac{1}{2}\|w\|^{2}+C\sum_{i=1}^{n}\xi_{i}\quad\text{while %
satisfying}\quad y_{i}(w^{T}x_{i}+b)\geq 1-\xi_{i}\quad\text{and}\quad\xi_{i}%
\geq 0\quad\text{for all }i" display="block"><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>ξ</mi></mrow></munder><mo>⁡</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><msup><mrow><mo>∥</mo><mi>w</mi><mo>∥</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>+</mo><mrow><mi>C</mi><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>ξ</mi><mi>i</mi></msub></mrow></mrow></mrow><mo mathvariant="italic" separator="true"> </mo><mtext>while satisfying</mtext><mo mathvariant="italic" separator="true"> </mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>w</mi><mi>T</mi></msup><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>≥</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>ξ</mi><mi>i</mi></msub></mrow></mrow><mo mathvariant="italic" separator="true"> </mo><mrow><mrow><mtext>and</mtext><mo mathvariant="italic" separator="true"> </mo><msub><mi>ξ</mi><mi>i</mi></msub></mrow><mo>≥</mo><mn>0</mn></mrow><mo mathvariant="italic" separator="true"> </mo><mrow><mtext>for all </mtext><mo>⁢</mo><mi>i</mi></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S9.SS1.SSS2.p1.m1" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> is the penalty term that controls the trade-off between the margin and the number of misclassified data points. The larger the value of <math id="S9.SS1.SSS2.p1.m2" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math>, the more the model will try to classify all data points correctly.</p>
</div>
</section>
</section>
<section id="S9.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>Kernel SVM</h3>

<div id="S9.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The limitation of linear SVM is that it can only find a linear decision boundary. To find a non-linear decision boundary, we can use kernel SVM.</p>
</div>
<div id="S9.SS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The core idea of kernel SVM is to map the input features into a higher-dimensional space, where the data points are linearly separable. In other words, each data point <math id="S9.SS2.p2.m1" class="ltx_Math" alttext="(x_{i},y_{i})\in\mathbb{R}^{d}\times\{-1,1\}" display="inline"><mrow><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo>×</mo><mrow><mo stretchy="false">{</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow></mrow></math> is mapped into a higher-dimensional space <math id="S9.SS2.p2.m2" class="ltx_Math" alttext="(\phi(x_{i}),y_{i})\in\mathbb{R}^{D}\times\{-1,1\}" display="inline"><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><msup><mi>ℝ</mi><mi>D</mi></msup><mo>×</mo><mrow><mo stretchy="false">{</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow></mrow></math>, where <math id="S9.SS2.p2.m3" class="ltx_Math" alttext="D&gt;d" display="inline"><mrow><mi>D</mi><mo>&gt;</mo><mi>d</mi></mrow></math>.</p>
</div>
<div id="S9.SS2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">More directly, since the ultimate goal is to solve the dual problem, the kernel function modifies the dual problem equation. The dual problem of kernel SVM is:</p>
<table id="S9.Ex27" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.Ex27.m1" class="ltx_Math" alttext="\max_{\alpha}\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}%
\alpha_{i}\alpha_{j}y_{i}y_{j}K(x_{i},x_{j})\quad\text{while satisfying}\quad%
\alpha_{i}\geq 0\quad\sum_{i=1}^{n}\alpha_{i}y_{i}=0" display="block"><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>α</mi></munder><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>α</mi><mi>i</mi></msub></mrow></mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>α</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>y</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>y</mi><mi>j</mi></msub><mo>⁢</mo><mi>K</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo mathvariant="italic" separator="true"> </mo><mtext>while satisfying</mtext><mo mathvariant="italic" separator="true"> </mo><msub><mi>α</mi><mi>i</mi></msub></mrow><mo>≥</mo><mn>0</mn></mrow><mo mathvariant="italic" separator="true"> </mo><mrow><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S9.SS2.p3.m1" class="ltx_Math" alttext="K(x_{i},x_{j})" display="inline"><mrow><mi>K</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math> is the kernel function.</p>
</div>
<div id="S9.SS2.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">When <math id="S9.SS2.p4.m1" class="ltx_Math" alttext="K(x_{i},x_{j})=x_{i}^{T}x_{j}" display="inline"><mrow><mrow><mi>K</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>T</mi></msubsup><mo>⁢</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mrow></math>, the dual problem equation degenerates to the linear SVM. For non-linear decision boundaries, we can use different kernel functions, such as the polynomial kernel (<math id="S9.SS2.p4.m2" class="ltx_Math" alttext="K(x_{i},x_{j})=(x_{i}^{T}x_{j}+1)^{d}" display="inline"><mrow><mrow><mi>K</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>x</mi><mi>i</mi><mi>T</mi></msubsup><mo>⁢</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mi>d</mi></msup></mrow></math>) and the Gaussian kernel (<math id="S9.SS2.p4.m3" class="ltx_Math" alttext="K(x_{i},x_{j})=\exp(-\frac{\|x_{i}-x_{j}\|^{2}}{2\sigma^{2}})" display="inline"><mrow><mrow><mi>K</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>-</mo><mfrac><msup><mrow><mo>∥</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>∥</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>).</p>
</div>
</section>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Perceptron &amp; Neural Networks</h2>

<section id="S10.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1 </span>Perceptron</h3>

<div id="S10.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A perceptron is a simple supervised learning algorithm that is used for binary classification tasks. The goal of a perceptron is to find the hyperplane that separates the data points of different classes. The model is defined as:</p>
<table id="S10.Ex28" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S10.Ex28.m1" class="ltx_Math" alttext="y=\text{sign}(b+\sum_{i=1}^{d}w_{i}x_{i})" display="block"><mrow><mi>y</mi><mo>=</mo><mrow><mtext>sign</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>b</mi><mo>+</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S10.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The perceptron algorithm is an iterative optimization algorithm that is used to find the weights that minimize the loss function. The algorithm works as follows:</p>
<ol id="S10.I1" class="ltx_enumerate">
<li id="S10.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S10.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Initialize the weights <math id="S10.I1.i1.p1.m1" class="ltx_Math" alttext="b,w_{1},\ldots,w_{d}" display="inline"><mrow><mi>b</mi><mo>,</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>w</mi><mi>d</mi></msub></mrow></math> randomly.</p>
</div>
</li>
<li id="S10.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S10.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For each data point <math id="S10.I1.i2.p1.m1" class="ltx_Math" alttext="(x,y)" display="inline"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></math>, calculate the predicted value <math id="S10.I1.i2.p1.m2" class="ltx_Math" alttext="\hat{y}" display="inline"><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></math>.</p>
</div>
</li>
<li id="S10.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S10.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the predicted value is incorrect, update the weights: <math id="S10.I1.i3.p1.m1" class="ltx_Math" alttext="w_{i}\leftarrow w_{i}+(y-\hat{y})x_{i}" display="inline"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>←</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>-</mo><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow></math> and <math id="S10.I1.i3.p1.m2" class="ltx_Math" alttext="b\leftarrow b+(y-\hat{y})" display="inline"><mrow><mi>b</mi><mo>←</mo><mrow><mi>b</mi><mo>+</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>-</mo><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math> (sometimes we use a learning rate <math id="S10.I1.i3.p1.m3" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> to control the update).</p>
</div>
</li>
<li id="S10.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S10.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Repeat steps 2 and 3 until the weights converge.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S10.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2 </span>Neural Networks</h3>

<div id="S10.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Perceptron is powerful enough to represent many boolean functions. However, it is not powerful enough to represent XOR function. To solve this problem, we can use neural networks.</p>
</div>
<div id="S10.SS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">One major change in neural networks is that we add more layers between the input layer and the output layer. These layers are called hidden layers. Another change is that we use activation functions to introduce non-linearity to the model (sign function, also called step function, is not suitable since it is not differentiable, thus cannot be used in backpropagation).</p>
</div>
<section id="S10.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.2.1 </span>XOR Problem</h4>

<div id="S10.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The XOR problem is a classic example that shows the limitation of linear models, since its special property makes it impossible to be solved by a single perceptron.</p>
</div>
<div id="S10.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">However, we can solve the XOR problem using a neural network with one hidden layer. First, we transform the XOR function into:</p>
<table id="S10.Ex29" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S10.Ex29.m1" class="ltx_Math" alttext="x_{1}\oplus x_{2}\equiv(x_{1}\lor x_{2})\land\neg(x_{1}\land x_{2})" display="block"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>⊕</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mo>≡</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>∨</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo>∧</mo><mrow><mi mathvariant="normal">¬</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>∧</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S10.SS2.SSS1.p3" class="ltx_para">
<p class="ltx_p">Then, we can use a neural network with one hidden layer to represent the XOR function. Suppose the input value <math id="S10.SS2.SSS1.p3.m1" class="ltx_Math" alttext="x_{1},x_{2}\in\{0,1\}" display="inline"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow></math>, and the activation function outputs 1 if the input is positive and 0 otherwise. The neural network can be represented as:</p>
</div>
<figure id="S10.F8" class="ltx_figure"><img src="{{ site.baseurl }}/images/2024-07-27-Artificial-Intelligence-Final-Review-Note-fig9.png" id="S10.F8.g1" class="ltx_graphics ltx_centering" width="325" height="195" alt="A neural network that solves the XOR problem.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>A neural network that solves the XOR problem.</figcaption>
</figure>
</section>
<section id="S10.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.2.2 </span>Backpropagation</h4>

<div id="S10.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Since neural network introduce hidden layers, we need a new algorithm to determine how to update the weights for each layer (since no label is given for hidden layers). The backpropagation algorithm is used to update the weights of a neural network.</p>
</div>
<div id="S10.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The core idea for backpropagation is:</p>
<table id="S10.Ex30" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S10.Ex30.m1" class="ltx_Math" alttext="\frac{\partial\text{Loss}}{\partial z_{i}}=\sum_{j\in\text{children}(i)}\frac{\partial\text{Loss}}{\partial z_{j}}\frac{\partial z_{j}}{\partial z_{i}}" display="block"><mrow><mfrac><mrow><mo>∂</mo><mo>⁡</mo><mtext>Loss</mtext></mrow><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>z</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>∈</mo><mrow><mtext>children</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mrow><mfrac><mrow><mo>∂</mo><mo>⁡</mo><mtext>Loss</mtext></mrow><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>z</mi><mi>j</mi></msub></mrow></mfrac><mo>⁢</mo><mfrac><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>z</mi><mi>j</mi></msub></mrow><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>z</mi><mi>i</mi></msub></mrow></mfrac></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S10.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">This equation can be calculated layer by layer, starting from the output layer.</p>
</div>
</section>
</section>
</section>
<section id="S11" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Decision Trees</h2>

<div id="S11.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">A decision tree is build from top to bottom. At each step, we choose the best feature to split the data. The goal is to maximize the information gain, which is defined as:</p>
<table id="S11.Ex31" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S11.Ex31.m1" class="ltx_Math" alttext="\text{Information gain}=\text{Entropy}(\text{parent})-\sum_{i}\frac{|\text{child}_{i}|}{|\text{parent}|}\text{Entropy}(\text{child}_{i})" display="block"><mrow><mtext>Information gain</mtext><mo>=</mo><mrow><mrow><mtext>Entropy</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>parent</mtext><mo stretchy="false">)</mo></mrow></mrow><mo>-</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder><mrow><mfrac><mrow><mo stretchy="false">|</mo><msub><mtext>child</mtext><mi>i</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mtext>parent</mtext><mo stretchy="false">|</mo></mrow></mfrac><mo>⁢</mo><mtext>Entropy</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mtext>child</mtext><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S11.p1.m1" class="ltx_Math" alttext="\text{Entropy}(S)=-\sum_{i}p_{i}\log_{2}p_{i}" display="inline"><mrow><mrow><mtext>Entropy</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><msub><mo largeop="true" symmetric="true">∑</mo><mi>i</mi></msub><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>⁢</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></math> is the entropy of a set <math id="S11.p1.m2" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math>.</p>
</div>
<div id="S11.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Noticing that the entropy of a set reaches its maximum when all classes are equally distributed, and reaches its minimum when all data points belong to the same class. The more information gain, the lower the entropy of the child nodes, the purer the child nodes.</p>
</div>
<div id="S11.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">To avoid overfitting, we can prune the tree backward. Or we can early stop the tree building process. The former is better, since we can use a validation set to determine the best time to stop.</p>
</div>
<section id="S11.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CART</h5>

<div id="S11.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">CART is a variant of decision tree that uses the Gini impurity to measure the impurity of a set. The Gini impurity is defined as:</p>
<table id="S11.Ex32" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S11.Ex32.m1" class="ltx_Math" alttext="\text{Gini impurity}=1-\sum_{i}p_{i}^{2}" display="block"><mrow><mtext>Gini impurity</mtext><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder><msubsup><mi>p</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</section>
</section>
<section id="S12" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">12 </span>Naive Bayes</h2>

<div id="S12.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The Naive Bayes algorithm is a simple supervised learning algorithm that is used for classification tasks. The algorithm is based on Bayes’ theorem, which is defined as:</p>
<table id="S12.Ex33" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S12.Ex33.m1" class="ltx_Math" alttext="P(A|B)=\frac{P(B|A)P(A)}{P(B)}" display="block"><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>B</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>A</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S12.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">The Naive Bayes algorithm assumes that the features are conditionally independent given the class label, which means we can modify the Bayes’ theorem as:</p>
<table id="S12.Ex34" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S12.Ex34.m1" class="ltx_Math" alttext="P(y|x_{1},x_{2},\ldots,x_{d})=\frac{P(y)P(x_{1}|y)P(x_{2}|y)\ldots P(x_{d}|y)}%
{P(x_{1})P(x_{2})\ldots P(x_{d})}" display="block"><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>d</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>2</mn></msub><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mi>d</mi></msub><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>d</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S12.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">However, this assumption is not always true. Hence, the algorithm is called "naive". Despite this, the Naive Bayes algorithm is still widely used in practice due to its simplicity and efficiency.</p>
</div>
<div id="S12.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">The Naive Bayes algorithm works as follows:</p>
<ol id="S12.I1" class="ltx_enumerate">
<li id="S12.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S12.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Given a training dataset with <math id="S12.I1.i1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> data points and <math id="S12.I1.i1.p1.m2" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> features, where the <math id="S12.I1.i1.p1.m3" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>-th data point is represented as <math id="S12.I1.i1.p1.m4" class="ltx_Math" alttext="(x_{i1},x_{i2},\ldots,x_{id},y_{i})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>d</mi></mrow></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math>, calculate the prior probability <math id="S12.I1.i1.p1.m5" class="ltx_Math" alttext="P(y)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow></math> and the conditional probability <math id="S12.I1.i1.p1.m6" class="ltx_Math" alttext="P(x_{j}|y)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mi>j</mi></msub><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> for each feature <math id="S12.I1.i1.p1.m7" class="ltx_Math" alttext="x_{j}" display="inline"><msub><mi>x</mi><mi>j</mi></msub></math>.</p>
</div>
</li>
<li id="S12.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S12.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For a new data point <math id="S12.I1.i2.p1.m1" class="ltx_Math" alttext="(x_{1},x_{2},\ldots,x_{d})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>d</mi></msub><mo stretchy="false">)</mo></mrow></math>, calculate the posterior probability <math id="S12.I1.i2.p1.m2" class="ltx_Math" alttext="P(y|x_{1},x_{2},\ldots,x_{d})" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>d</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math> for each class label <math id="S12.I1.i2.p1.m3" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>.</p>
</div>
</li>
</ol>
</div>
<div id="S12.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">
<span class="ltx_text ltx_font_bold ltx_framed_rectangle">Example:</span>
</p>
</div>
<div id="S12.p6" class="ltx_para">
<p class="ltx_p">For a simple 2 feature dataset with 6 data points:</p>
</div>
<figure id="S12.tab1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">Feature 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">Feature 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">Class Label</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">1</td>
<td class="ltx_td ltx_align_center ltx_border_t">1</td>
<td class="ltx_td ltx_align_center ltx_border_t">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">1</td>
<td class="ltx_td ltx_align_center">0</td>
<td class="ltx_td ltx_align_center">1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">0</td>
<td class="ltx_td ltx_align_center">1</td>
<td class="ltx_td ltx_align_center">1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">0</td>
<td class="ltx_td ltx_align_center">1</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">1</td>
<td class="ltx_td ltx_align_center">1</td>
<td class="ltx_td ltx_align_center">1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb">1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S12.p7" class="ltx_para ltx_noindent">
<p class="ltx_p">The prior probability <math id="S12.p7.m1" class="ltx_Math" alttext="P(y)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow></math> is:</p>
<table id="S14.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S12.Ex35"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S12.Ex35.m1" class="ltx_Math" alttext="\displaystyle P(y=0)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex35.m2" class="ltx_Math" alttext="\displaystyle=\frac{2}{6}=\frac{1}{3}" display="inline"><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>2</mn><mn>6</mn></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>3</mn></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S12.Ex36"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S12.Ex36.m1" class="ltx_Math" alttext="\displaystyle P(y=1)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex36.m2" class="ltx_Math" alttext="\displaystyle=\frac{4}{6}=\frac{2}{3}" display="inline"><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>4</mn><mn>6</mn></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>2</mn><mn>3</mn></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S12.p8" class="ltx_para ltx_noindent">
<p class="ltx_p">The conditional probability <math id="S12.p8.m1" class="ltx_Math" alttext="P(x_{j}|y)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mi>j</mi></msub><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> is:</p>
<table id="S14.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S12.Ex37"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S12.Ex37.m1" class="ltx_Math" alttext="\displaystyle P(x_{1}=1|y=0)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mrow><mn>1</mn><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex37.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}" display="inline"><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S12.Ex38"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S12.Ex38.m1" class="ltx_Math" alttext="\displaystyle P(x_{1}=1|y=1)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mrow><mn>1</mn><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex38.m2" class="ltx_Math" alttext="\displaystyle=\frac{3}{4}" display="inline"><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>3</mn><mn>4</mn></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S12.Ex39"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S12.Ex39.m1" class="ltx_Math" alttext="\displaystyle P(x_{2}=1|y=0)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mrow><mn>1</mn><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex39.m2" class="ltx_Math" alttext="\displaystyle=1" display="inline"><mrow><mi></mi><mo>=</mo><mn>1</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S12.Ex40"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S12.Ex40.m1" class="ltx_Math" alttext="\displaystyle P(x_{2}=1|y=1)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mrow><mn>1</mn><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex40.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}" display="inline"><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S12.p9" class="ltx_para ltx_noindent">
<p class="ltx_p">For a new data point <math id="S12.p9.m1" class="ltx_Math" alttext="(x_{1}=1,x_{2}=1)" display="inline"><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow></mrow><mo stretchy="false">)</mo></mrow></math>, the posterior probability <math id="S12.p9.m2" class="ltx_Math" alttext="P(y|x_{1},x_{2})" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math> is:</p>
<table id="S14.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S12.Ex41"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S12.Ex41.m1" class="ltx_Math" alttext="\displaystyle P(y=0|x_{1}=1,x_{2}=1)" display="inline"><mrow><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>0</mn><mo stretchy="false">|</mo><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex41.m2" class="ltx_Math" alttext="\displaystyle\propto P(y=0)P(x_{1}=1|y=0)P(x_{2}=1|y=0)" display="inline"><mrow><mi></mi><mo>∝</mo><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mrow><mn>1</mn><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mrow><mn>1</mn><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S12.Ex42"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex42.m1" class="ltx_Math" alttext="\displaystyle=\frac{1}{3}\times\frac{1}{2}\times 1=\frac{1}{6}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>3</mn></mfrac></mstyle><mo>×</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>×</mo><mn>1</mn></mrow><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>6</mn></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S12.Ex43"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S12.Ex43.m1" class="ltx_Math" alttext="\displaystyle P(y=1|x_{1}=1,x_{2}=1)" display="inline"><mrow><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex43.m2" class="ltx_Math" alttext="\displaystyle\propto P(y=1)P(x_{1}=1|y=1)P(x_{2}=1|y=1)" display="inline"><mrow><mi></mi><mo>∝</mo><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mrow><mn>1</mn><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mrow><mn>1</mn><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>y</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S12.Ex44"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S12.Ex44.m1" class="ltx_Math" alttext="\displaystyle=\frac{2}{3}\times\frac{3}{4}\times\frac{1}{2}=\frac{1}{4}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>2</mn><mn>3</mn></mfrac></mstyle><mo>×</mo><mstyle displaystyle="true"><mfrac><mn>3</mn><mn>4</mn></mfrac></mstyle><mo>×</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>4</mn></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S12.p10" class="ltx_para ltx_noindent">
<p class="ltx_p">Since <math id="S12.p10.m1" class="ltx_Math" alttext="P(y=1|x_{1}=1,x_{2}=1)&gt;P(y=0|x_{1}=1,x_{2}=1)" display="inline"><mrow><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo>&gt;</mo><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>0</mn><mo stretchy="false">|</mo><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></math>, the predicted class label is 1.</p>
</div>
</section>
<section id="S13" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">13 </span>Ensemble Learning</h2>

<div id="S13.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">An ensemble method combines the predictions of multiple models to improve the overall performance. Some representative ensemble methods include bagging, boosting, and ramdom forest.</p>
</div>
<section id="S13.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">13.1 </span>Boosting</h3>

<div id="S13.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The core idea of boosting is to train multiple weak learners sequentially, where each weak learner is trained to correct the mistakes of the previous weak learners.</p>
</div>
<div id="S13.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Firstly, we define the error of a weak learner as:</p>
<table id="S13.Ex45" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S13.Ex45.m1" class="ltx_Math" alttext="\text{Error}=\sum_{i=1}^{n}w_{i}\mathbb{I}(y_{i}\neq\hat{y}_{i})" display="block"><mrow><mtext>Error</mtext><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><mi>𝕀</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>≠</mo><msub><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S13.SS1.p2.m1" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is the weight of the <math id="S13.SS1.p2.m2" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>-th data point, <math id="S13.SS1.p2.m3" class="ltx_Math" alttext="y_{i}" display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the true label, <math id="S13.SS1.p2.m4" class="ltx_Math" alttext="\hat{y}_{i}" display="inline"><msub><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mi>i</mi></msub></math> is the predicted label, and <math id="S13.SS1.p2.m5" class="ltx_Math" alttext="\mathbb{I}(\cdot)" display="inline"><mrow><mi>𝕀</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow></mrow></math> is the indicator function (1 if the condition is true, 0 otherwise).</p>
</div>
<div id="S13.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">Then, we can calculate the weight of the weak learner as:</p>
<table id="S13.Ex46" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S13.Ex46.m1" class="ltx_Math" alttext="\text{Weight}=\frac{1}{2}\log\left(\frac{1-\text{Error}}{\text{Error}}\right)" display="block"><mrow><mtext>Weight</mtext><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mtext>Error</mtext></mrow><mtext>Error</mtext></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S13.SS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">We the update the weight of each data point as:</p>
<table id="S13.Ex47" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S13.Ex47.m1" class="ltx_Math" alttext="w_{i}\leftarrow w_{i}\exp(-\text{Weight}\times y_{i}\times\hat{y}_{i})" display="block"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>←</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>-</mo><mrow><mtext>Weight</mtext><mo>×</mo><msub><mi>y</mi><mi>i</mi></msub><mo>×</mo><msub><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mi>i</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div id="S13.SS1.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">The workflow of boosting is:</p>
<ol id="S13.I1" class="ltx_enumerate">
<li id="S13.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S13.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Initialize the weights of the data points as <math id="S13.I1.i1.p1.m1" class="ltx_Math" alttext="w_{i}=\frac{1}{n}" display="inline"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow></math>.</p>
</div>
</li>
<li id="S13.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S13.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For <math id="S13.I1.i2.p1.m1" class="ltx_Math" alttext="t=1,2,\ldots,T" display="inline"><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>T</mi></mrow></mrow></math>:</p>
<ol id="S13.I1.i2.I1" class="ltx_enumerate">
<li id="S13.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S13.I1.i2.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Train a weak learner <math id="S13.I1.i2.I1.i1.p1.m1" class="ltx_Math" alttext="G_{t}" display="inline"><msub><mi>G</mi><mi>t</mi></msub></math> based on the weighted data points.</p>
</div>
</li>
<li id="S13.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S13.I1.i2.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Calculate the error of <math id="S13.I1.i2.I1.i2.p1.m1" class="ltx_Math" alttext="G_{t}" display="inline"><msub><mi>G</mi><mi>t</mi></msub></math>.</p>
</div>
</li>
<li id="S13.I1.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S13.I1.i2.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Calculate the weight of <math id="S13.I1.i2.I1.i3.p1.m1" class="ltx_Math" alttext="G_{t}" display="inline"><msub><mi>G</mi><mi>t</mi></msub></math>.</p>
</div>
</li>
<li id="S13.I1.i2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="S13.I1.i2.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Update the weights of the data points based on the predictions of <math id="S13.I1.i2.I1.i4.p1.m1" class="ltx_Math" alttext="G_{t}" display="inline"><msub><mi>G</mi><mi>t</mi></msub></math>.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S13.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S13.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Combine the weak learners using the weights.</p>
</div>
</li>
</ol>
</div>
<div id="S13.SS1.p6" class="ltx_para ltx_noindent">
<p class="ltx_p">For the final prediction, we can use the weighted sum of the predictions of the weak learners:</p>
<table id="S13.Ex48" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S13.Ex48.m1" class="ltx_Math" alttext="\hat{y}=\text{sign}\left(\sum_{t=1}^{T}\text{Weight}_{t}G_{t}(x)\right)" display="block"><mrow><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mo>=</mo><mrow><mtext>sign</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mtext>Weight</mtext><mi>t</mi></msub><mo>⁢</mo><msub><mi>G</mi><mi>t</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</section>
<section id="S13.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">13.2 </span>Bagging</h3>

<div id="S13.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The core idea of bagging is to train multiple models in parallel, where each model is trained on a random subset of the data points. The final prediction is the average of the predictions of the models.</p>
</div>
</section>
<section id="S13.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">13.3 </span>Random Forest</h3>

<div id="S13.SS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Random forest is an ensemble method that combines the ideas of bagging and decision trees. The core idea of random forest is to train multiple decision trees in parallel, where each decision tree is trained on a random subset of features.</p>
</div>
</section>
</section>
<section id="S14" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">14 </span>Clustering</h2>

<div id="S14.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Clustering is an unsupervised learning algorithm that is used to group similar data points together. Some representative clustering algorithms include K-means, DBSCAN, and hierarchical clustering.</p>
</div>
<section id="S14.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">14.1 </span>K-Means</h3>

<div id="S14.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The K-means algorithm is a simple clustering algorithm that is used to partition the data points into <math id="S14.SS1.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> clusters. The algorithm works as follows:</p>
<ol id="S14.I1" class="ltx_enumerate">
<li id="S14.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S14.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Initialize the centroids of the <math id="S14.I1.i1.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> clusters randomly.</p>
</div>
</li>
<li id="S14.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S14.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Assign each data point to the nearest centroid.</p>
</div>
</li>
<li id="S14.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S14.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Update the centroids by taking the average of the data points in each cluster.</p>
</div>
</li>
<li id="S14.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S14.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Repeat steps 2 and 3 until the centroids converge.</p>
</div>
</li>
</ol>
</div>
<section id="S14.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">14.1.1 </span>How to Choose <math id="S14.SS1.SSS1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>
</h4>

<section id="S14.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Elbow method</h5>

<div id="S14.SS1.SSS1.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">We can use the elbow method to determine the optimal number of clusters, which means we plot the loss function (e.g., the sum of squared errors) against the number of clusters and choose the number of clusters that corresponds to the "elbow" point.</p>
</div>
</section>
<section id="S14.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">G-means</h5>

<div id="S14.SS1.SSS1.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">G-means is a variant of K-means that uses a statistical test to determine the optimal number of clusters. The algorithm works as follows:</p>
<ol id="S14.I2" class="ltx_enumerate">
<li id="S14.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S14.I2.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Initialize <math id="S14.I2.i1.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> as a small number.</p>
</div>
</li>
<li id="S14.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S14.I2.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Run K-means with <math id="S14.I2.i2.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> clusters.
</p>
</div>
</li>
<li id="S14.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S14.I2.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">For each cluster, perform a statistical test to determine if the cluster fits a Gaussian distribution.</p>
</div>
</li>
<li id="S14.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S14.I2.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">If the cluster does not fit a Gaussian distribution, split the cluster into two sub-clusters.</p>
</div>
</li>
<li id="S14.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S14.I2.i5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Repeat steps 2 to 4 until all clusters fit a Gaussian distribution.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S14.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">14.1.2 </span>How to Evaluate Clustering</h4>

<section id="S14.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Internal Evaluation</h5>

<div id="S14.SS1.SSS2.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Internal evaluation metrics consider the distance between data points within the same cluster and the distance between data points in different clusters. Some common internal evaluation metrics include the silhouette score and the Davies-Bouldin index.</p>
</div>
</section>
<section id="S14.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">External Evaluation</h5>

<div id="S14.SS1.SSS2.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">External evaluation metrics compare the clustering results with the ground truth labels. Some common external evaluation metrics include the adjusted Rand index and the Fowlkes-Mallows index.</p>
</div>
</section>
</section>
<section id="S14.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">14.1.3 </span>How to Cluster Non-Circular Data</h4>

<div id="S14.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Use kernel K-means, which maps the data points into a higher-dimensional space where the data points are linearly separable. Other methods include spectral clustering and DBSCAN.</p>
</div>
</section>
<section id="S14.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">14.1.4 </span>How to Initialize Centroids</h4>

<div id="S14.SS1.SSS4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The initialization of centroids can affect the performance of the K-means algorithm. Some common methods for initializing centroids include:</p>
<ul id="S14.I3" class="ltx_itemize">
<li id="S14.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S14.I3.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Initialize the centroids randomly.</p>
</div>
</li>
<li id="S14.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S14.I3.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Choose each centroid furthest from the previous centroids.</p>
</div>
</li>
<li id="S14.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S14.I3.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Try multiple initializations and choose the one with the lowest loss.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
</article>
</div>
</div>
</body>
</html>
